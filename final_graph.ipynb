{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Notebook for BTCGraphGuard\n",
    "\n",
    "**Authors: Xuhui Zhan, Tianhao Qu, Siyu Yang**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:48:15.088529Z",
     "start_time": "2025-04-07T01:48:15.084030Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.utils import from_networkx\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T00:55:21.950839Z",
     "start_time": "2025-04-07T00:55:18.838962Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare Data\n",
    "data_root = 'data/elliptic_bitcoin_dataset'\n",
    "elliptic_txs_features = pd.read_csv(os.path.join(data_root, 'elliptic_txs_features.csv'), header=None)\n",
    "elliptic_txs_edgelist = pd.read_csv(os.path.join(data_root, 'elliptic_txs_edgelist.csv'))\n",
    "elliptic_txs_classes = pd.read_csv(os.path.join(data_root, 'elliptic_txs_classes.csv'))\n",
    "\n",
    "elliptic_txs_features.columns = ['txId'] + [f'V{i}' for i in range(1, 167)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T00:55:23.908129Z",
     "start_time": "2025-04-07T00:55:23.905040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(203769, 167)\n",
      "(234355, 2)\n",
      "(203769, 2)\n"
     ]
    }
   ],
   "source": [
    "print(elliptic_txs_features.shape)\n",
    "print(elliptic_txs_edgelist.shape)\n",
    "print(elliptic_txs_classes.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:09:33.195956Z",
     "start_time": "2025-04-07T01:09:33.171203Z"
    }
   },
   "outputs": [],
   "source": [
    "elliptic_txs_classes['class_mapped'] = elliptic_txs_classes['class'].replace({'1': 'illicit', '2': 'licit'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:09:34.482266Z",
     "start_time": "2025-04-07T01:09:33.723713Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "G = nx.from_pandas_edgelist(elliptic_txs_edgelist, 'txId1', 'txId2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random seed settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:09:36.155117Z",
     "start_time": "2025-04-07T01:09:36.152694Z"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "NUM_EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:09:36.907170Z",
     "start_time": "2025-04-07T01:09:36.903171Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed_for_torch(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)      # For single-GPU.\n",
    "        torch.cuda.manual_seed_all(seed)  # For multi-GPU.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "def set_seed_for_numpy(seed):\n",
    "    np.random.seed(seed) \n",
    "    \n",
    "def set_seed_for_random(seed):\n",
    "    random.seed(seed)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:09:37.807822Z",
     "start_time": "2025-04-07T01:09:37.798822Z"
    }
   },
   "outputs": [],
   "source": [
    "set_seed_for_torch(RANDOM_STATE)\n",
    "set_seed_for_numpy(RANDOM_STATE)\n",
    "set_seed_for_random(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T21:45:13.356208Z",
     "start_time": "2025-04-05T21:45:13.345483Z"
    }
   },
   "outputs": [],
   "source": [
    "# Spaceholders for EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:10:22.940488Z",
     "start_time": "2025-04-07T01:10:22.742410Z"
    }
   },
   "outputs": [],
   "source": [
    "tx_id_mapping = {tx_id: idx for idx, tx_id in enumerate(elliptic_txs_features['txId'])}\n",
    "\n",
    "# Create an explicit copy of the filtered DataFrame\n",
    "edges_with_features = elliptic_txs_edgelist[elliptic_txs_edgelist['txId1'].isin(list(tx_id_mapping.keys())) & \n",
    "                                           elliptic_txs_edgelist['txId2'].isin(list(tx_id_mapping.keys()))].copy()\n",
    "\n",
    "# Now use loc to set values (though with copy() above, direct assignment would also work)\n",
    "edges_with_features.loc[:, 'Id1'] = edges_with_features['txId1'].map(tx_id_mapping)\n",
    "edges_with_features.loc[:, 'Id2'] = edges_with_features['txId2'].map(tx_id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:10:23.654842Z",
     "start_time": "2025-04-07T01:10:23.548299Z"
    }
   },
   "outputs": [],
   "source": [
    "edge_index = torch.tensor(edges_with_features[['Id1', 'Id2']].values.T, dtype=torch.long)\n",
    "node_features = torch.tensor(elliptic_txs_features.drop(columns=['txId']).values, \n",
    "                             dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:10:24.115669Z",
     "start_time": "2025-04-07T01:10:24.093341Z"
    }
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "class_labels = le.fit_transform(elliptic_txs_classes['class'])\n",
    "node_labels = torch.tensor(class_labels, dtype=torch.long)\n",
    "original_labels = le.inverse_transform(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:10:24.812006Z",
     "start_time": "2025-04-07T01:10:24.809006Z"
    }
   },
   "outputs": [],
   "source": [
    "data = Data(x=node_features, \n",
    "            edge_index=edge_index, \n",
    "            y=node_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:10:25.511512Z",
     "start_time": "2025-04-07T01:10:25.508672Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:10:28.084388Z",
     "start_time": "2025-04-07T01:10:28.078385Z"
    }
   },
   "outputs": [],
   "source": [
    "known_mask   = (data.y == 0) | (data.y == 1)  # Only nodes with known labels licit or illicit\n",
    "unknown_mask = data.y == 2                    # Nodes with unknown labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:10:29.067259Z",
     "start_time": "2025-04-07T01:10:29.059912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations per split\n",
      "    Training   :     37,251 (80.00 %)\n",
      "    Validation :      4,656 (10.00 %)\n",
      "    Testing    :      4,657 (10.00 %)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_known_nodes = known_mask.sum().item()\n",
    "permutations = torch.randperm(num_known_nodes)\n",
    "train_size = int(0.8 * num_known_nodes)\n",
    "val_size = int(0.1 * num_known_nodes)\n",
    "test_size = num_known_nodes - train_size - val_size\n",
    "\n",
    "total = np.sum([train_size, val_size, test_size])\n",
    "\n",
    "print(f\"\"\"Number of observations per split\n",
    "    Training   : {train_size:10,} ({100*train_size/total:0.2f} %)\n",
    "    Validation : {val_size:10,} ({100*val_size/total:0.2f} %)\n",
    "    Testing    : {test_size:10,} ({100*test_size/total:0.2f} %)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T01:10:29.957245Z",
     "start_time": "2025-04-07T01:10:29.941750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203769\n"
     ]
    }
   ],
   "source": [
    "data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "data.val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "data.test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "\n",
    "train_indices = known_mask.nonzero(as_tuple=True)[0][permutations[:train_size]]\n",
    "val_indices = known_mask.nonzero(as_tuple=True)[0][permutations[train_size:train_size + val_size]]\n",
    "test_indices = known_mask.nonzero(as_tuple=True)[0][permutations[train_size + val_size:]]\n",
    "\n",
    "data.train_mask[train_indices] = True\n",
    "data.val_mask[val_indices] = True\n",
    "data.test_mask[test_indices] = True\n",
    "\n",
    "print(len(data.train_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x', 'edge_index', 'train_mask', 'test_mask', 'y', 'val_mask']\n"
     ]
    }
   ],
   "source": [
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph attention network (GAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, heads=8):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=0.6)\n",
    "        self.conv2 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False, dropout=0.6)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(input_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gnn_model(graph_data, checkpoint_path, model_args=None, num_epochs=200, lr=0.005, weight_decay=5e-4, verbose=True):\n",
    "    \"\"\"\n",
    "    Train a Graph Neural Network model and save checkpoints.\n",
    "    \n",
    "    Args:\n",
    "        graph_data (torch_geometric.data.Data): The prepared graph data\n",
    "        checkpoint_path (str): Path to save model checkpoints\n",
    "        model_args (dict, optional): Dictionary containing model parameters:\n",
    "            - model_name: Type of GNN model ('GAT' or 'GraphSAGE')\n",
    "            - input_dim: Input feature dimension\n",
    "            - hidden_dim: Hidden layer dimension\n",
    "            - output_dim: Output dimension (number of classes)\n",
    "            - heads: Number of attention heads (for GAT, default: 8)\n",
    "        num_epochs (int): Maximum number of training epochs\n",
    "        lr (float): Learning rate for Adam optimizer\n",
    "        weight_decay (float): Weight decay for regularization\n",
    "        verbose (bool): Whether to print training progress\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing trained model and training metrics\n",
    "    \"\"\"\n",
    "    # Set up device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "    \n",
    "    # Move data to device\n",
    "    graph_data = graph_data.to(device)\n",
    "    \n",
    "    # Set default model args if not provided\n",
    "    if model_args is None:\n",
    "        model_args = {\n",
    "            'model_name': 'GAT',  # Default to GAT if not specified\n",
    "            'input_dim': graph_data.x.shape[1],\n",
    "            'hidden_dim': 64,\n",
    "            'output_dim': len(torch.unique(graph_data.y[graph_data.y != 2])),  # Exclude unknown class\n",
    "            'heads': 8\n",
    "        }\n",
    "    \n",
    "    # Initialize model based on model_name\n",
    "    model_name = model_args.get('model_name', 'GAT')\n",
    "    \n",
    "    if model_name.upper() == 'GAT':\n",
    "        model = GAT(\n",
    "            input_dim=model_args['input_dim'],\n",
    "            hidden_dim=model_args['hidden_dim'],\n",
    "            output_dim=model_args['output_dim'],\n",
    "            heads=model_args.get('heads', 8)\n",
    "        ).to(device)\n",
    "        model_type = 'GAT'\n",
    "    elif model_name.upper() == 'GRAPHSAGE' or model_name.upper() == 'SAGE':\n",
    "        model = GraphSAGE(\n",
    "            input_dim=model_args['input_dim'],\n",
    "            hidden_dim=model_args['hidden_dim'],\n",
    "            output_dim=model_args['output_dim']\n",
    "        ).to(device)\n",
    "        model_type = 'GraphSAGE'\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_name}. Use 'GAT' or 'GraphSAGE'.\")\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training function\n",
    "    def train_step():\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph_data)\n",
    "        loss = criterion(out[graph_data.train_mask], graph_data.y[graph_data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    # Evaluation function\n",
    "    def evaluate(mask):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(graph_data)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct = pred[mask] == graph_data.y[mask]\n",
    "            acc = int(correct.sum()) / int(mask.sum())\n",
    "        return acc\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0\n",
    "    best_model_state = None\n",
    "    train_history = {\n",
    "        'losses': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        loss = train_step()\n",
    "        train_acc = evaluate(graph_data.train_mask)\n",
    "        val_acc = evaluate(graph_data.val_mask)\n",
    "        \n",
    "        train_history['losses'].append(loss)\n",
    "        train_history['train_acc'].append(train_acc)\n",
    "        train_history['val_acc'].append(val_acc)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict()\n",
    "            torch.save(best_model_state, checkpoint_path)\n",
    "            \n",
    "        if verbose and epoch % 10 == 0:\n",
    "            print(f'{model_type} Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Load best model and evaluate\n",
    "    model.load_state_dict(best_model_state)\n",
    "    test_acc = evaluate(graph_data.test_mask)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'{model_type} Test Accuracy: {test_acc:.4f}')\n",
    "        print(f'{model_type} Training Time: {training_time:.2f} seconds')\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'best_model_state': best_model_state,\n",
    "        'test_accuracy': test_acc,\n",
    "        'val_accuracy': best_val_acc,\n",
    "        'training_time': training_time,\n",
    "        'training_history': train_history\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_model_results(results, save_dir=None, model_name=\"model\"):\n",
    "    \"\"\"\n",
    "    Inspects and visualizes the results from the train_gnn_model function.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): Results dictionary returned by train_gnn_model\n",
    "        save_dir (str, optional): Directory to save visualizations and metrics\n",
    "                                 If None, extracts directory from checkpoint path\n",
    "        model_name (str): Name of the model for labeling plots and files\n",
    "    \"\"\"\n",
    "    # Extract training history and metrics\n",
    "    history = results['training_history']\n",
    "    test_acc = results['test_accuracy']\n",
    "    val_acc = results['val_accuracy']\n",
    "    training_time = results['training_time']\n",
    "    \n",
    "    # Determine save directory\n",
    "    if save_dir is None:\n",
    "        if 'best_model_state' in results and isinstance(results['best_model_state'], str):\n",
    "            save_dir = os.path.dirname(results['best_model_state'])\n",
    "        else:\n",
    "            save_dir = 'output'\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Create figure with 2 subplots\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    epochs = range(1, len(history['losses']) + 1)\n",
    "    plt.plot(epochs, history['losses'], 'bo-', label='Training Loss')\n",
    "    plt.title(f'{model_name} - Training Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot training and validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['train_acc'], 'go-', label='Training Accuracy')\n",
    "    plt.plot(epochs, history['val_acc'], 'ro-', label='Validation Accuracy')\n",
    "    plt.title(f'{model_name} - Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Save figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f'{model_name}_training_history.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Print and save metrics\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Test Accuracy': f'{test_acc:.4f}',\n",
    "        'Validation Accuracy': f'{val_acc:.4f}',\n",
    "        'Training Time (s)': f'{training_time:.2f}',\n",
    "        'Final Training Loss': f'{history[\"losses\"][-1]:.4f}',\n",
    "        'Number of Epochs': len(history['losses'])\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*20} {model_name} Results {'='*20}\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    pd.DataFrame([metrics]).to_csv(os.path.join(save_dir, f'{model_name}_metrics.csv'), index=False)\n",
    "    \n",
    "    # Create additional visualization: accuracy vs loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(history['losses'], history['val_acc'], c=range(len(history['losses'])), cmap='viridis', \n",
    "                s=100, alpha=0.7, edgecolors='black', linewidth=1)\n",
    "    plt.colorbar(label='Epoch')\n",
    "    plt.title(f'{model_name} - Validation Accuracy vs Training Loss')\n",
    "    plt.xlabel('Training Loss')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.grid(True, linestyle='--', alpha=0.3)\n",
    "    plt.savefig(os.path.join(save_dir, f'{model_name}_acc_vs_loss.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dim = data.x.shape[1]\n",
    "# hidden_dim = 64\n",
    "# output_dim = len(torch.unique(data.y[data.y != 2]))\n",
    "\n",
    "# gat_output_dir = 'output/GAT'\n",
    "\n",
    "# # For GAT model\n",
    "# gat_args = {\n",
    "#     'model_name': 'GAT',\n",
    "#     'input_dim': input_dim,\n",
    "#     'hidden_dim': hidden_dim,\n",
    "#     'output_dim': output_dim,\n",
    "#     'heads': 8\n",
    "# }\n",
    "\n",
    "# gat_results = train_gnn_model(\n",
    "#     graph_data=data,\n",
    "#     checkpoint_path=os.path.join(gat_output_dir, \"gat_best_model.pt\"),\n",
    "#     model_args=gat_args,\n",
    "#     num_epochs=200,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sage_output_dir = 'output/GraphSAGE'\n",
    "\n",
    "# # For GraphSAGE model\n",
    "# sage_args = {\n",
    "#     'model_name': 'GraphSAGE',\n",
    "#     'input_dim': input_dim,\n",
    "#     'hidden_dim': hidden_dim,\n",
    "#     'output_dim': output_dim\n",
    "# }\n",
    "\n",
    "# sage_results = train_gnn_model(\n",
    "#     graph_data=data,\n",
    "#     checkpoint_path=os.path.join(sage_output_dir, \"sage_best_model.pt\"),\n",
    "#     model_args=sage_args,\n",
    "#     num_epochs=200,\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# metrics = inspect_model_results(sage_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmented Graph labels by self-supervised-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_labels(data, percentage=0.3, model=None):\n",
    "    \"\"\"\n",
    "    Augments the training data by adding predicted labels for previously unknown nodes.\n",
    "    \n",
    "    Args:\n",
    "        data (torch_geometric.data.Data): The input graph data\n",
    "        percentage (float, optional): Percentage of unknown nodes to add to training. Defaults to 0.3.\n",
    "        model (torch.nn.Module, optional): Pre-trained model to use for predictions. If None,\n",
    "                                          uses label propagation for self-supervised learning.\n",
    "    \n",
    "    Returns:\n",
    "        torch_geometric.data.Data: Augmented data with expanded training set\n",
    "    \"\"\"\n",
    "    # Create a copy of the data to avoid modifying the original\n",
    "    augmented_data = data.clone()\n",
    "    device = augmented_data.x.device\n",
    "    \n",
    "    # Get masks for known and unknown labels\n",
    "    known_mask = (augmented_data.y == 0) | (augmented_data.y == 1)\n",
    "    unknown_mask = augmented_data.y == 2\n",
    "    \n",
    "    # Count unknown nodes and calculate how many to add\n",
    "    num_unknown = unknown_mask.sum().item()\n",
    "    num_to_add = int(num_unknown * percentage)\n",
    "    \n",
    "    if model is None:\n",
    "        # Self-supervised learning approach using simple propagation\n",
    "        print(f\"Using self-supervised learning for label augmentation\")\n",
    "        \n",
    "        # Create graph from data\n",
    "        G = nx.Graph()\n",
    "        edge_index = augmented_data.edge_index.cpu().numpy()\n",
    "        \n",
    "        # Add nodes and edges to the graph\n",
    "        for i in range(augmented_data.num_nodes):\n",
    "            G.add_node(i)\n",
    "            \n",
    "        for i in range(edge_index.shape[1]):\n",
    "            G.add_edge(edge_index[0, i], edge_index[1, i])\n",
    "        \n",
    "        # Simple label propagation implementation\n",
    "        predicted_labels = torch.zeros_like(augmented_data.y)\n",
    "        confidence_scores = torch.zeros(len(augmented_data.y))\n",
    "        \n",
    "        # For each unknown node, check neighbors' labels\n",
    "        for node in range(len(augmented_data.y)):\n",
    "            if unknown_mask[node]:\n",
    "                # Get all neighbors\n",
    "                neighbors = list(G.neighbors(node))\n",
    "                if not neighbors:\n",
    "                    continue\n",
    "                \n",
    "                # Count labels of neighbors\n",
    "                label_count = {0: 0, 1: 0}\n",
    "                neighbor_features = []\n",
    "                \n",
    "                for neighbor in neighbors:\n",
    "                    if known_mask[neighbor]:\n",
    "                        neighbor_label = augmented_data.y[neighbor].item()\n",
    "                        label_count[neighbor_label] = label_count.get(neighbor_label, 0) + 1\n",
    "                        neighbor_features.append(augmented_data.x[neighbor])\n",
    "                \n",
    "                # If we have neighbors with known labels\n",
    "                if sum(label_count.values()) > 0:\n",
    "                    # Assign most frequent label\n",
    "                    if label_count[0] > label_count[1]:\n",
    "                        predicted_labels[node] = 0\n",
    "                        confidence_scores[node] = label_count[0] / (label_count[0] + label_count[1])\n",
    "                    elif label_count[1] > 0:\n",
    "                        predicted_labels[node] = 1\n",
    "                        confidence_scores[node] = label_count[1] / (label_count[0] + label_count[1])\n",
    "                else:\n",
    "                    # If no neighbors have known labels, look at second-degree neighbors\n",
    "                    second_degree_neighbors = []\n",
    "                    for neighbor in neighbors:\n",
    "                        second_degree_neighbors.extend(list(G.neighbors(neighbor)))\n",
    "                    \n",
    "                    # Remove duplicates and the original node\n",
    "                    second_degree_neighbors = list(set(second_degree_neighbors))\n",
    "                    if node in second_degree_neighbors:\n",
    "                        second_degree_neighbors.remove(node)\n",
    "                    \n",
    "                    # Count labels of second-degree neighbors\n",
    "                    second_label_count = {0: 0, 1: 0}\n",
    "                    for neighbor in second_degree_neighbors:\n",
    "                        if known_mask[neighbor]:\n",
    "                            neighbor_label = augmented_data.y[neighbor].item()\n",
    "                            second_label_count[neighbor_label] = second_label_count.get(neighbor_label, 0) + 1\n",
    "                    \n",
    "                    # Assign most frequent label from second-degree neighbors\n",
    "                    if sum(second_label_count.values()) > 0:\n",
    "                        if second_label_count[0] > second_label_count[1]:\n",
    "                            predicted_labels[node] = 0\n",
    "                            confidence_scores[node] = second_label_count[0] / (second_label_count[0] + second_label_count[1])\n",
    "                        elif second_label_count[1] > 0:\n",
    "                            predicted_labels[node] = 1\n",
    "                            confidence_scores[node] = second_label_count[1] / (second_label_count[0] + second_label_count[1])\n",
    "    else:\n",
    "        # Use the provided model for predictions\n",
    "        print(f\"Using provided model for label augmentation\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(augmented_data)\n",
    "            probabilities = F.softmax(out, dim=1)\n",
    "            \n",
    "            # Get the highest probability and corresponding class\n",
    "            confidence_scores, predicted_labels = probabilities.max(dim=1)\n",
    "    \n",
    "    # Select the top confident predictions among unknown nodes\n",
    "    unknown_indices = unknown_mask.nonzero(as_tuple=True)[0]\n",
    "    unknown_confidence = confidence_scores[unknown_indices]\n",
    "    \n",
    "    # Sort by confidence\n",
    "    sorted_indices = unknown_confidence.argsort(descending=True)\n",
    "    top_indices = unknown_indices[sorted_indices[:num_to_add]]\n",
    "    \n",
    "    # Update labels and training mask for selected nodes\n",
    "    augmented_data.y[top_indices] = predicted_labels[top_indices]\n",
    "    augmented_data.train_mask[top_indices] = True\n",
    "    \n",
    "    # Print statistics\n",
    "    added_illicit = (predicted_labels[top_indices] == 1).sum().item()\n",
    "    added_licit = (predicted_labels[top_indices] == 0).sum().item()\n",
    "    \n",
    "    print(f\"Added {num_to_add} previously unknown nodes to training set:\")\n",
    "    print(f\"  - Predicted illicit: {added_illicit} ({100*added_illicit/num_to_add:.2f}%)\")\n",
    "    print(f\"  - Predicted licit: {added_licit} ({100*added_licit/num_to_add:.2f}%)\")\n",
    "    \n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment_data = augment_labels(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sage_model = GraphSAGE(input_dim, hidden_dim, output_dim)\n",
    "# sage_model.load_state_dict(torch.load(os.path.join(sage_output_dir, \"sage_best_model.pt\")))\n",
    "# augment_data_sage = augment_labels(data, model=sage_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subgraph Property Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_graph_properties(data, model=None, save_dir='output/graph_analysis', verbose=True):\n",
    "    \"\"\"\n",
    "    Analyzes subgraph properties for different node types (train/val/test/unknown) and labels.\n",
    "    \n",
    "    Args:\n",
    "        data (torch_geometric.data.Data): The graph data object\n",
    "        model (torch.nn.Module, optional): Model to predict unknown node labels\n",
    "        save_dir (str): Directory to save results and plots\n",
    "        verbose (bool): Whether to print detailed information\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing all computed metrics\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import networkx as nx\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from torch_geometric.utils import to_networkx\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert to NetworkX graph for analysis\n",
    "    G = to_networkx(data, to_undirected=True)\n",
    "    \n",
    "    # Define label names\n",
    "    label_names = {\n",
    "        0: 'licit',\n",
    "        1: 'illicit',\n",
    "        2: 'unknown'\n",
    "    }\n",
    "    \n",
    "    # Create dictionaries to store metrics\n",
    "    metrics = {}\n",
    "    node_metrics = []\n",
    "    \n",
    "    # Calculate degree centrality for all nodes (to avoid recalculation)\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    betweenness_centrality = nx.betweenness_centrality(G, k=min(100, len(G.nodes())), seed=42)\n",
    "    \n",
    "    try:\n",
    "        closeness_centrality = nx.closeness_centrality(G)\n",
    "    except:\n",
    "        if verbose:\n",
    "            print(\"Warning: Could not compute closeness centrality for the full graph (likely disconnected)\")\n",
    "        closeness_centrality = {node: 0 for node in G.nodes()}\n",
    "    \n",
    "    try:\n",
    "        eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)\n",
    "    except:\n",
    "        if verbose:\n",
    "            print(\"Warning: Eigenvector centrality did not converge, using approximate method\")\n",
    "        try:\n",
    "            eigenvector_centrality = nx.eigenvector_centrality_numpy(G)\n",
    "        except:\n",
    "            if verbose:\n",
    "                print(\"Warning: Could not compute eigenvector centrality, using zeros\")\n",
    "            eigenvector_centrality = {node: 0 for node in G.nodes()}\n",
    "            \n",
    "    clustering_coefficients = nx.clustering(G)\n",
    "    \n",
    "    # First, analyze known nodes (train/val/test) by their actual labels\n",
    "    for split_name in ['train', 'val', 'test']:\n",
    "        split_mask = getattr(data, f'{split_name}_mask')\n",
    "        \n",
    "        for label in [0, 1]:  # 0: licit, 1: illicit\n",
    "            # Create mask for this split+label combination\n",
    "            combined_mask = split_mask & (data.y == label)\n",
    "            node_indices = combined_mask.nonzero(as_tuple=True)[0].cpu().numpy()\n",
    "            \n",
    "            if len(node_indices) == 0:\n",
    "                if verbose:\n",
    "                    print(f\"No nodes found for {split_name}_{label_names[label]}\")\n",
    "                continue\n",
    "            \n",
    "            # Name for this subgroup\n",
    "            group_name = f\"{split_name}_{label_names[label]}\"\n",
    "            \n",
    "            # Process this subgroup\n",
    "            process_node_group(G, node_indices, group_name, metrics, node_metrics,\n",
    "                             degree_centrality, betweenness_centrality, \n",
    "                             closeness_centrality, eigenvector_centrality,\n",
    "                             clustering_coefficients, verbose)\n",
    "    \n",
    "    # Then, analyze unknown nodes, splitting by predicted labels if model is provided\n",
    "    unknown_mask = (data.y == 2)\n",
    "    unknown_indices = unknown_mask.nonzero(as_tuple=True)[0]\n",
    "    \n",
    "    if len(unknown_indices) > 0:\n",
    "        if model is None:\n",
    "            # If no model, just analyze unknown nodes as one group\n",
    "            group_name = \"unknown\"\n",
    "            process_node_group(G, unknown_indices.cpu().numpy(), group_name, metrics, node_metrics,\n",
    "                             degree_centrality, betweenness_centrality, \n",
    "                             closeness_centrality, eigenvector_centrality,\n",
    "                             clustering_coefficients, verbose)\n",
    "        else:\n",
    "            # Use model to predict labels for unknown nodes\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                out = model(data)\n",
    "                pred_probs = F.softmax(out, dim=1)\n",
    "                pred_labels = pred_probs.argmax(dim=1)\n",
    "                \n",
    "                # Get indices for unknown nodes predicted as licit/illicit\n",
    "                pred_unknown_licit = unknown_mask & (pred_labels == 0)\n",
    "                pred_unknown_illicit = unknown_mask & (pred_labels == 1)\n",
    "                \n",
    "                # Process predicted licit unknown nodes\n",
    "                pred_licit_indices = pred_unknown_licit.nonzero(as_tuple=True)[0].cpu().numpy()\n",
    "                if len(pred_licit_indices) > 0:\n",
    "                    group_name = \"pred_unknown_licit\"\n",
    "                    process_node_group(G, pred_licit_indices, group_name, metrics, node_metrics,\n",
    "                                     degree_centrality, betweenness_centrality, \n",
    "                                     closeness_centrality, eigenvector_centrality,\n",
    "                                     clustering_coefficients, verbose)\n",
    "                    \n",
    "                # Process predicted illicit unknown nodes\n",
    "                pred_illicit_indices = pred_unknown_illicit.nonzero(as_tuple=True)[0].cpu().numpy()\n",
    "                if len(pred_illicit_indices) > 0:\n",
    "                    group_name = \"pred_unknown_illicit\"\n",
    "                    process_node_group(G, pred_illicit_indices, group_name, metrics, node_metrics,\n",
    "                                     degree_centrality, betweenness_centrality, \n",
    "                                     closeness_centrality, eigenvector_centrality,\n",
    "                                     clustering_coefficients, verbose)\n",
    "                \n",
    "                # Print statistics about predictions\n",
    "                if verbose:\n",
    "                    total_unknown = len(unknown_indices)\n",
    "                    pred_licit_count = len(pred_licit_indices)\n",
    "                    pred_illicit_count = len(pred_illicit_indices)\n",
    "                    print(f\"Unknown nodes ({total_unknown}) predicted as:\")\n",
    "                    print(f\"  - Licit: {pred_licit_count} ({100*pred_licit_count/total_unknown:.2f}%)\")\n",
    "                    print(f\"  - Illicit: {pred_illicit_count} ({100*pred_illicit_count/total_unknown:.2f}%)\")\n",
    "    \n",
    "    # Create a DataFrame with all node-level metrics\n",
    "    node_df = pd.DataFrame(node_metrics)\n",
    "    \n",
    "    # Save node-level metrics\n",
    "    node_df.to_csv(os.path.join(save_dir, 'node_metrics.csv'), index=False)\n",
    "    \n",
    "    # Save graph-level metrics\n",
    "    graph_metrics_df = pd.DataFrame.from_dict(metrics, orient='index')\n",
    "    graph_metrics_df.to_csv(os.path.join(save_dir, 'graph_metrics.csv'))\n",
    "    \n",
    "    # Create visualizations\n",
    "    create_visualizations(metrics, node_df, save_dir)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Analysis complete. Results saved to {save_dir}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def process_node_group(G, node_indices, group_name, metrics, node_metrics,\n",
    "                     degree_centrality, betweenness_centrality, \n",
    "                     closeness_centrality, eigenvector_centrality,\n",
    "                     clustering_coefficients, verbose):\n",
    "    \"\"\"Helper function to process a group of nodes and calculate metrics\"\"\"\n",
    "    \n",
    "    # Extract the subgraph\n",
    "    subgraph = G.subgraph(node_indices)\n",
    "    \n",
    "    # Calculate graph-level metrics\n",
    "    group_metrics = {\n",
    "        'num_nodes': len(subgraph),\n",
    "        'num_edges': subgraph.number_of_edges(),\n",
    "        'density': nx.density(subgraph),\n",
    "        'avg_degree': np.mean([d for _, d in subgraph.degree()]) if len(subgraph) > 0 else 0,\n",
    "        'avg_clustering': np.mean([clustering_coefficients.get(node, 0) for node in subgraph.nodes()]),\n",
    "    }\n",
    "    \n",
    "    # Calculate connected components\n",
    "    connected_components = list(nx.connected_components(subgraph))\n",
    "    group_metrics['num_components'] = len(connected_components)\n",
    "    \n",
    "    if len(connected_components) > 0:\n",
    "        largest_cc_size = max([len(cc) for cc in connected_components])\n",
    "        group_metrics['largest_component_size'] = largest_cc_size\n",
    "        group_metrics['largest_component_ratio'] = largest_cc_size / len(subgraph) if len(subgraph) > 0 else 0\n",
    "    else:\n",
    "        group_metrics['largest_component_size'] = 0\n",
    "        group_metrics['largest_component_ratio'] = 0\n",
    "    \n",
    "    # Calculate centrality averages\n",
    "    group_metrics['avg_degree_centrality'] = np.mean([degree_centrality.get(node, 0) for node in subgraph.nodes()])\n",
    "    group_metrics['avg_betweenness_centrality'] = np.mean([betweenness_centrality.get(node, 0) for node in subgraph.nodes()])\n",
    "    group_metrics['avg_closeness_centrality'] = np.mean([closeness_centrality.get(node, 0) for node in subgraph.nodes()])\n",
    "    group_metrics['avg_eigenvector_centrality'] = np.mean([eigenvector_centrality.get(node, 0) for node in subgraph.nodes()])\n",
    "    \n",
    "    # Add homophily measure - how many edges connect to same-group nodes\n",
    "    internal_edges = 0\n",
    "    external_edges = 0\n",
    "    \n",
    "    node_set = set(node_indices)\n",
    "    for node in subgraph:\n",
    "        for neighbor in G.neighbors(node):\n",
    "            if neighbor in node_set:\n",
    "                internal_edges += 1\n",
    "            else:\n",
    "                external_edges += 1\n",
    "    \n",
    "    # Each internal edge is counted twice (once from each end)\n",
    "    internal_edges = internal_edges / 2\n",
    "    group_metrics['internal_edges'] = internal_edges\n",
    "    group_metrics['external_edges'] = external_edges\n",
    "    group_metrics['homophily'] = internal_edges / (internal_edges + external_edges) if (internal_edges + external_edges) > 0 else 0\n",
    "    \n",
    "    # Store the metrics for this group\n",
    "    metrics[group_name] = group_metrics\n",
    "    \n",
    "    # Also store node-level metrics for later visualization\n",
    "    for node in subgraph.nodes():\n",
    "        node_metrics.append({\n",
    "            'node_id': node,\n",
    "            'group': group_name,\n",
    "            'degree': G.degree(node),\n",
    "            'degree_centrality': degree_centrality.get(node, 0),\n",
    "            'betweenness_centrality': betweenness_centrality.get(node, 0),\n",
    "            'closeness_centrality': closeness_centrality.get(node, 0),\n",
    "            'eigenvector_centrality': eigenvector_centrality.get(node, 0),\n",
    "            'clustering_coefficient': clustering_coefficients.get(node, 0)\n",
    "        })\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Processed {group_name}: {len(subgraph)} nodes, {subgraph.number_of_edges()} edges\")\n",
    "\n",
    "def create_visualizations(metrics, node_df, save_dir):\n",
    "    \"\"\"Create and save visualizations of the graph metrics\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    \n",
    "    # Set the style\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    \n",
    "    # Convert metrics to DataFrame for easier plotting\n",
    "    metrics_df = pd.DataFrame.from_dict(metrics, orient='index')\n",
    "    \n",
    "    # 1. Centrality comparison between groups\n",
    "    centrality_metrics = ['avg_degree_centrality', 'avg_betweenness_centrality', \n",
    "                          'avg_closeness_centrality', 'avg_eigenvector_centrality']\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    metrics_df[centrality_metrics].plot(kind='bar', figsize=(15, 6))\n",
    "    plt.title('Average Centrality Metrics by Group')\n",
    "    plt.ylabel('Centrality Value')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'centrality_by_group.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Homophily and connectivity\n",
    "    connectivity_metrics = ['density', 'homophily', 'avg_clustering', 'largest_component_ratio']\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    metrics_df[connectivity_metrics].plot(kind='bar', figsize=(15, 6))\n",
    "    plt.title('Connectivity Metrics by Group')\n",
    "    plt.ylabel('Value')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'connectivity_by_group.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Node and edge count\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = metrics_df[['num_nodes', 'num_edges']].plot(kind='bar', figsize=(15, 6))\n",
    "    plt.title('Node and Edge Count by Group')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%.0f')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'node_edge_count.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Scatter plot of centrality metrics for nodes\n",
    "    centrality_pairs = [\n",
    "        ('degree_centrality', 'betweenness_centrality'),\n",
    "        ('degree_centrality', 'eigenvector_centrality'),\n",
    "        ('closeness_centrality', 'eigenvector_centrality')\n",
    "    ]\n",
    "    \n",
    "    # Define a color map for consistent group coloring\n",
    "    group_order = node_df['group'].unique()\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(group_order)))\n",
    "    group_colors = {group: colors[i] for i, group in enumerate(group_order)}\n",
    "    \n",
    "    for x_metric, y_metric in centrality_pairs:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        for group in group_order:\n",
    "            group_data = node_df[node_df['group'] == group]\n",
    "            plt.scatter(\n",
    "                group_data[x_metric], \n",
    "                group_data[y_metric],\n",
    "                alpha=0.6, \n",
    "                label=group,\n",
    "                color=group_colors[group],\n",
    "                s=50\n",
    "            )\n",
    "        \n",
    "        plt.title(f'{y_metric.replace(\"_\", \" \").title()} vs {x_metric.replace(\"_\", \" \").title()}')\n",
    "        plt.xlabel(x_metric.replace(\"_\", \" \").title())\n",
    "        plt.ylabel(y_metric.replace(\"_\", \" \").title())\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_dir, f'scatter_{x_metric}_vs_{y_metric}.png'), dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # 5. Distribution of degree centrality by group\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for group in group_order:\n",
    "        group_data = node_df[node_df['group'] == group]\n",
    "        sns.kdeplot(group_data['degree_centrality'], label=group)\n",
    "    \n",
    "    plt.title('Distribution of Degree Centrality by Group')\n",
    "    plt.xlabel('Degree Centrality')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'degree_centrality_distribution.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 6. Heatmap of metrics correlation\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation = metrics_df.corr()\n",
    "    mask = np.triu(np.ones_like(correlation, dtype=bool))\n",
    "    sns.heatmap(correlation, mask=mask, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "    plt.title('Correlation Between Graph Metrics')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'metrics_correlation.png'), dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Analysis with original data\n",
    "# original_metrics = analyze_graph_properties(\n",
    "#     data, \n",
    "#     save_dir='output/graph_analysis/original',\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# # Analysis with model predictions\n",
    "# sage_metrics = analyze_graph_properties(\n",
    "#     data,\n",
    "#     model=sage_model,  # Using SAGE model for predictions\n",
    "#     save_dir='output/graph_analysis/model_predictions',\n",
    "#     verbose=True\n",
    "# )\n",
    "\n",
    "# Compare metrics between original and predicted\n",
    "def compare_metrics(original_metrics, predicted_metrics, save_path='output/graph_analysis/metrics_comparison.csv'):\n",
    "    \"\"\"Compare metrics between original and predicted data\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Create DataFrames\n",
    "    original_df = pd.DataFrame.from_dict(original_metrics, orient='index')\n",
    "    predicted_df = pd.DataFrame.from_dict(predicted_metrics, orient='index')\n",
    "    \n",
    "    # Common groups (like train_licit, train_illicit, etc.)\n",
    "    common_groups = set(original_df.index).intersection(set(predicted_df.index))\n",
    "    \n",
    "    # Compare metrics for common groups\n",
    "    comparison = {}\n",
    "    for group in common_groups:\n",
    "        group_comparison = {}\n",
    "        for metric in original_df.columns:\n",
    "            original_value = original_df.loc[group, metric]\n",
    "            predicted_value = predicted_df.loc[group, metric]\n",
    "            difference = predicted_value - original_value\n",
    "            percent_change = (difference / original_value) * 100 if original_value != 0 else float('inf')\n",
    "            \n",
    "            group_comparison[f\"{metric}_original\"] = original_value\n",
    "            group_comparison[f\"{metric}_predicted\"] = predicted_value\n",
    "            group_comparison[f\"{metric}_diff\"] = difference\n",
    "            group_comparison[f\"{metric}_pct_change\"] = percent_change\n",
    "            \n",
    "        comparison[group] = group_comparison\n",
    "    \n",
    "    # Convert to DataFrame and save\n",
    "    comparison_df = pd.DataFrame.from_dict(comparison, orient='index')\n",
    "    comparison_df.to_csv(save_path)\n",
    "    \n",
    "    print(f\"Metrics comparison saved to {save_path}\")\n",
    "    return comparison_df\n",
    "\n",
    "# # Execute comparison\n",
    "# comparison = compare_metrics(original_metrics, sage_metrics)\n",
    "\n",
    "# # Print key findings\n",
    "# print(\"\\n==== KEY FINDINGS ====\")\n",
    "# print(\"Top metrics differences between original and predicted data:\")\n",
    "\n",
    "# # Find top 5 metrics with largest percent changes\n",
    "# for group in comparison.index:\n",
    "#     pct_change_cols = [col for col in comparison.columns if col.endswith('_pct_change')]\n",
    "#     largest_changes = comparison.loc[group, pct_change_cols].abs().nlargest(5)\n",
    "    \n",
    "#     print(f\"\\nGroup: {group}\")\n",
    "#     for metric in largest_changes.index:\n",
    "#         base_metric = metric.replace('_pct_change', '')\n",
    "#         original = comparison.loc[group, f\"{base_metric}_original\"]\n",
    "#         predicted = comparison.loc[group, f\"{base_metric}_predicted\"]\n",
    "#         pct_change = comparison.loc[group, metric]\n",
    "        \n",
    "#         change_direction = \"increased\" if pct_change > 0 else \"decreased\"\n",
    "#         print(f\"  {base_metric}: {original:.4f}  {predicted:.4f} ({change_direction} by {abs(pct_change):.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_subgraph_properties(\n",
    "    original_metrics, \n",
    "    predicted_metrics, \n",
    "    save_dir,\n",
    "    prefix=\"comparison\",\n",
    "    key_metrics=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare subgraph properties between licit and illicit nodes across different datasets.\n",
    "    \n",
    "    Args:\n",
    "        original_metrics (dict): Metrics from original known nodes\n",
    "        predicted_metrics (dict): Metrics from nodes with predicted labels\n",
    "        save_dir (str): Directory to save comparison results\n",
    "        prefix (str): Prefix for saved files\n",
    "        key_metrics (list, optional): Key metrics to focus on\n",
    "        \n",
    "    Returns:\n",
    "        dict: Comparison results\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Define key metrics if not provided\n",
    "    if key_metrics is None:\n",
    "        key_metrics = [\n",
    "            'homophily', 'density', 'avg_clustering', 'avg_degree_centrality',\n",
    "            'avg_betweenness_centrality', 'largest_component_ratio'\n",
    "        ]\n",
    "    \n",
    "    # Extract metrics for licit and illicit nodes\n",
    "    original_licit = {}\n",
    "    original_illicit = {}\n",
    "    predicted_licit = {}\n",
    "    predicted_illicit = {}\n",
    "    \n",
    "    # Find relevant groups in original metrics\n",
    "    for group, metrics in original_metrics.items():\n",
    "        if 'licit' in group:\n",
    "            if 'illicit' in group:\n",
    "                original_illicit[group] = metrics\n",
    "            else:\n",
    "                original_licit[group] = metrics\n",
    "    \n",
    "    # Find relevant groups in predicted metrics\n",
    "    for group, metrics in predicted_metrics.items():\n",
    "        if 'pred' in group or 'unknown' in group:\n",
    "            if 'illicit' in group:\n",
    "                predicted_illicit[group] = metrics\n",
    "            elif 'licit' in group:\n",
    "                predicted_licit[group] = metrics\n",
    "    \n",
    "    # Prepare data for comparison\n",
    "    comparison_data = []\n",
    "    \n",
    "    # Compare key metrics between original and predicted\n",
    "    for metric in key_metrics:\n",
    "        # Original licit metrics (average across train/val/test if available)\n",
    "        orig_licit_values = [m.get(metric, 0) for m in original_licit.values() if metric in m]\n",
    "        orig_licit_avg = np.mean(orig_licit_values) if orig_licit_values else 0\n",
    "        \n",
    "        # Original illicit metrics\n",
    "        orig_illicit_values = [m.get(metric, 0) for m in original_illicit.values() if metric in m]\n",
    "        orig_illicit_avg = np.mean(orig_illicit_values) if orig_illicit_values else 0\n",
    "        \n",
    "        # Predicted licit metrics\n",
    "        pred_licit_values = [m.get(metric, 0) for m in predicted_licit.values() if metric in m]\n",
    "        pred_licit_avg = np.mean(pred_licit_values) if pred_licit_values else 0\n",
    "        \n",
    "        # Predicted illicit metrics\n",
    "        pred_illicit_values = [m.get(metric, 0) for m in predicted_illicit.values() if metric in m]\n",
    "        pred_illicit_avg = np.mean(pred_illicit_values) if pred_illicit_values else 0\n",
    "        \n",
    "        # Calculate similarity scores\n",
    "        licit_similarity = 1 - min(1, abs(pred_licit_avg - orig_licit_avg) / max(0.0001, abs(orig_licit_avg)))\n",
    "        illicit_similarity = 1 - min(1, abs(pred_illicit_avg - orig_illicit_avg) / max(0.0001, abs(orig_illicit_avg)))\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'metric': metric,\n",
    "            'original_licit': orig_licit_avg,\n",
    "            'original_illicit': orig_illicit_avg,\n",
    "            'predicted_licit': pred_licit_avg,\n",
    "            'predicted_illicit': pred_illicit_avg,\n",
    "            'licit_similarity': licit_similarity,\n",
    "            'illicit_similarity': illicit_similarity,\n",
    "            'overall_similarity': (licit_similarity + illicit_similarity) / 2\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Save comparison data\n",
    "    df.to_csv(os.path.join(save_dir, f\"{prefix}_subgraph_comparison.csv\"), index=False)\n",
    "    \n",
    "    # Create visualizations\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot comparison of key metrics\n",
    "    for i, metric in enumerate(key_metrics):\n",
    "        if i >= len(df):\n",
    "            continue\n",
    "            \n",
    "        metric_data = df[df['metric'] == metric].iloc[0]\n",
    "        \n",
    "        plt.subplot(3, 2, i+1)\n",
    "        \n",
    "        # Create grouped bar chart\n",
    "        labels = ['Original', 'Predicted']\n",
    "        licit_values = [metric_data['original_licit'], metric_data['predicted_licit']]\n",
    "        illicit_values = [metric_data['original_illicit'], metric_data['predicted_illicit']]\n",
    "        \n",
    "        x = np.arange(len(labels))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, licit_values, width, label='Licit', color='green')\n",
    "        plt.bar(x + width/2, illicit_values, width, label='Illicit', color='red')\n",
    "        \n",
    "        plt.xlabel('Data Source')\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(f'{metric} Comparison (Similarity: {metric_data[\"overall_similarity\"]:.2f})')\n",
    "        plt.xticks(x, labels)\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f\"{prefix}_metrics_comparison.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Calculate overall similarity\n",
    "    overall_similarity = df['overall_similarity'].mean()\n",
    "    \n",
    "    # Create summary plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(df['metric'], df['overall_similarity'], color='blue')\n",
    "    plt.title(f'Subgraph Property Similarity (Overall: {overall_similarity:.4f})')\n",
    "    plt.xlabel('Metric')\n",
    "    plt.ylabel('Similarity Score')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f\"{prefix}_overall_similarity.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'comparison_data': df,\n",
    "        'overall_similarity': overall_similarity\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_graph_analysis(\n",
    "    data, \n",
    "    output_dir='output/comprehensive_analysis',\n",
    "    model_configs=None,\n",
    "    augment_percentage=0.3,\n",
    "    epochs=100,\n",
    "    random_seed=42,\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs comprehensive analysis of graph augmentation methods.\n",
    "    \n",
    "    Args:\n",
    "        data (torch_geometric.data.Data): The input graph data\n",
    "        output_dir (str): Root directory for saving results\n",
    "        model_configs (dict, optional): Model configurations\n",
    "        augment_percentage (float): Percentage of unknown nodes to add (default: 0.3)\n",
    "        epochs (int): Number of training epochs\n",
    "        random_seed (int): Random seed for reproducibility\n",
    "        verbose (bool): Whether to print detailed information\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing all results and comparisons\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from datetime import datetime\n",
    "    import random\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    def set_seed_for_torch(seed):\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    def set_seed_for_numpy(seed):\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def set_seed_for_random(seed):\n",
    "        random.seed(seed)\n",
    "    \n",
    "    # Set random seeds\n",
    "    set_seed_for_torch(random_seed)\n",
    "    set_seed_for_numpy(random_seed)\n",
    "    set_seed_for_random(random_seed)\n",
    "    \n",
    "    # Create output directory\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = f\"{output_dir}_{timestamp}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Setup default model configurations if not provided\n",
    "    if model_configs is None:\n",
    "        model_configs = {\n",
    "            'input_dim': data.x.shape[1],\n",
    "            'hidden_dim': 64,\n",
    "            'output_dim': len(torch.unique(data.y[data.y != 2])),\n",
    "            'gat_heads': 8\n",
    "        }\n",
    "    \n",
    "    # Create subdirectories\n",
    "    original_dir = os.path.join(output_dir, \"original\")\n",
    "    models_dir = os.path.join(output_dir, \"models\")\n",
    "    augmented_dir = os.path.join(output_dir, \"augmented\")\n",
    "    comparison_dir = os.path.join(output_dir, \"comparison\")\n",
    "    \n",
    "    for directory in [original_dir, models_dir, augmented_dir, comparison_dir]:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    # Dictionary to store all results\n",
    "    results = {\n",
    "        'original': {},\n",
    "        'base_models': {},\n",
    "        'augmented': {},\n",
    "        'augmented_models': {},\n",
    "        'comparison': {}\n",
    "    }\n",
    "    \n",
    "    # Step 1: Analyze original graph properties\n",
    "    if verbose:\n",
    "        print(\"\\n==== Step 1: Analyzing Original Graph Properties ====\")\n",
    "    \n",
    "    original_metrics = analyze_graph_properties(\n",
    "        data, \n",
    "        save_dir=os.path.join(original_dir, \"graph_properties\"),\n",
    "        verbose=verbose\n",
    "    )\n",
    "    results['original']['metrics'] = original_metrics\n",
    "    \n",
    "    # Step 2: Train GAT and GraphSAGE on known graph\n",
    "    if verbose:\n",
    "        print(\"\\n==== Step 2: Training Base Models ====\")\n",
    "    \n",
    "    # For GAT model\n",
    "    gat_args = {\n",
    "        'model_name': 'GAT',\n",
    "        'input_dim': model_configs['input_dim'],\n",
    "        'hidden_dim': model_configs['hidden_dim'],\n",
    "        'output_dim': model_configs['output_dim'],\n",
    "        'heads': model_configs['gat_heads']\n",
    "    }\n",
    "    \n",
    "    gat_path = os.path.join(models_dir, \"base_gat\")\n",
    "    os.makedirs(gat_path, exist_ok=True)\n",
    "    \n",
    "    base_gat_results = train_gnn_model(\n",
    "        graph_data=data,\n",
    "        checkpoint_path=os.path.join(gat_path, \"base_gat_model.pt\"),\n",
    "        model_args=gat_args,\n",
    "        num_epochs=epochs,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # For GraphSAGE model\n",
    "    sage_args = {\n",
    "        'model_name': 'GraphSAGE',\n",
    "        'input_dim': model_configs['input_dim'],\n",
    "        'hidden_dim': model_configs['hidden_dim'],\n",
    "        'output_dim': model_configs['output_dim']\n",
    "    }\n",
    "    \n",
    "    sage_path = os.path.join(models_dir, \"base_sage\")\n",
    "    os.makedirs(sage_path, exist_ok=True)\n",
    "    \n",
    "    base_sage_results = train_gnn_model(\n",
    "        graph_data=data,\n",
    "        checkpoint_path=os.path.join(sage_path, \"base_sage_model.pt\"),\n",
    "        model_args=sage_args,\n",
    "        num_epochs=epochs,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    # Step 3: Inspect results and analyze model properties\n",
    "    if verbose:\n",
    "        print(\"\\n==== Step 3: Inspecting Base Model Results ====\")\n",
    "        \n",
    "    gat_metrics = inspect_model_results(base_gat_results, save_dir=gat_path, model_name=\"base_gat\")\n",
    "    sage_metrics = inspect_model_results(base_sage_results, save_dir=sage_path, model_name=\"base_sage\")\n",
    "    \n",
    "    results['base_models']['gat'] = {\n",
    "        'model': base_gat_results['model'],\n",
    "        'metrics': gat_metrics,\n",
    "        'training_history': base_gat_results['training_history']\n",
    "    }\n",
    "    \n",
    "    results['base_models']['sage'] = {\n",
    "        'model': base_sage_results['model'],\n",
    "        'metrics': sage_metrics,\n",
    "        'training_history': base_sage_results['training_history']\n",
    "    }\n",
    "    \n",
    "    # Analyze graph properties with base models\n",
    "    gat_model = base_gat_results['model']\n",
    "    sage_model = base_sage_results['model']\n",
    "    \n",
    "    gat_graph_metrics = analyze_graph_properties(\n",
    "        data, \n",
    "        model=gat_model,\n",
    "        save_dir=os.path.join(gat_path, \"graph_properties\"),\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    sage_graph_metrics = analyze_graph_properties(\n",
    "        data, \n",
    "        model=sage_model,\n",
    "        save_dir=os.path.join(sage_path, \"graph_properties\"),\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    results['base_models']['gat']['graph_metrics'] = gat_graph_metrics\n",
    "    results['base_models']['sage']['graph_metrics'] = sage_graph_metrics\n",
    "    \n",
    "    # Step 4: Augment data using different methods\n",
    "    if verbose:\n",
    "        print(\"\\n==== Step 4: Augmenting Data with Different Methods ====\")\n",
    "    \n",
    "    # Method 1: Label propagation (default method)\n",
    "    lp_augmented_data = augment_labels(data, percentage=augment_percentage)\n",
    "    \n",
    "    # Method 2: GAT-based augmentation\n",
    "    gat_augmented_data = augment_labels(data, percentage=augment_percentage, model=gat_model)\n",
    "    \n",
    "    # Method 3: GraphSAGE-based augmentation\n",
    "    sage_augmented_data = augment_labels(data, percentage=augment_percentage, model=sage_model)\n",
    "    \n",
    "    # Store augmented data\n",
    "    augmented_data = {\n",
    "        'label_propagation': lp_augmented_data,\n",
    "        'gat': gat_augmented_data,\n",
    "        'sage': sage_augmented_data\n",
    "    }\n",
    "    results['augmented']['data'] = augmented_data\n",
    "    \n",
    "    # Step 5: Analyze graph properties for each augmented method\n",
    "    if verbose:\n",
    "        print(\"\\n==== Step 5: Analyzing Augmented Graph Properties ====\")\n",
    "    \n",
    "    for method_name, aug_data in augmented_data.items():\n",
    "        method_dir = os.path.join(augmented_dir, method_name)\n",
    "        os.makedirs(method_dir, exist_ok=True)\n",
    "        \n",
    "        # Analyze without model predictions\n",
    "        aug_metrics = analyze_graph_properties(\n",
    "            aug_data, \n",
    "            save_dir=os.path.join(method_dir, \"graph_properties\"),\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        results['augmented'][f'{method_name}_metrics'] = aug_metrics\n",
    "    \n",
    "    # Step 6: Train models on augmented graphs\n",
    "    if verbose:\n",
    "        print(\"\\n==== Step 6: Training Models on Augmented Graphs ====\")\n",
    "    \n",
    "    augmented_models = {}\n",
    "    \n",
    "    for method_name, aug_data in augmented_data.items():\n",
    "        method_dir = os.path.join(augmented_dir, method_name)\n",
    "        \n",
    "        # Train GAT on augmented data\n",
    "        gat_aug_path = os.path.join(method_dir, \"gat\")\n",
    "        os.makedirs(gat_aug_path, exist_ok=True)\n",
    "        \n",
    "        gat_aug_results = train_gnn_model(\n",
    "            graph_data=aug_data,\n",
    "            checkpoint_path=os.path.join(gat_aug_path, f\"{method_name}_gat_model.pt\"),\n",
    "            model_args=gat_args,\n",
    "            num_epochs=epochs,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        # Train GraphSAGE on augmented data\n",
    "        sage_aug_path = os.path.join(method_dir, \"sage\")\n",
    "        os.makedirs(sage_aug_path, exist_ok=True)\n",
    "        \n",
    "        sage_aug_results = train_gnn_model(\n",
    "            graph_data=aug_data,\n",
    "            checkpoint_path=os.path.join(sage_aug_path, f\"{method_name}_sage_model.pt\"),\n",
    "            model_args=sage_args,\n",
    "            num_epochs=epochs,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        # Inspect model results\n",
    "        gat_aug_metrics = inspect_model_results(\n",
    "            gat_aug_results, \n",
    "            save_dir=gat_aug_path, \n",
    "            model_name=f\"{method_name}_gat\"\n",
    "        )\n",
    "        \n",
    "        sage_aug_metrics = inspect_model_results(\n",
    "            sage_aug_results, \n",
    "            save_dir=sage_aug_path, \n",
    "            model_name=f\"{method_name}_sage\"\n",
    "        )\n",
    "        \n",
    "        # Store model results\n",
    "        augmented_models[f'{method_name}_gat'] = {\n",
    "            'model': gat_aug_results['model'],\n",
    "            'metrics': gat_aug_metrics,\n",
    "            'training_history': gat_aug_results['training_history']\n",
    "        }\n",
    "        \n",
    "        augmented_models[f'{method_name}_sage'] = {\n",
    "            'model': sage_aug_results['model'],\n",
    "            'metrics': sage_aug_metrics,\n",
    "            'training_history': sage_aug_results['training_history']\n",
    "        }\n",
    "    \n",
    "    results['augmented_models'] = augmented_models\n",
    "    \n",
    "    # Step 7: Analyze graph properties using augmented models\n",
    "    if verbose:\n",
    "        print(\"\\n==== Step 7: Analyzing Graph Properties with Augmented Models ====\")\n",
    "    \n",
    "    for model_name, model_info in augmented_models.items():\n",
    "        model = model_info['model']\n",
    "        method_name = model_name.split('_')[0]\n",
    "        model_type = model_name.split('_')[1]\n",
    "        \n",
    "        save_dir = os.path.join(augmented_dir, method_name, model_type, \"graph_properties_with_model\")\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Analyze graph properties using this model\n",
    "        graph_metrics = analyze_graph_properties(\n",
    "            data,  # Use original data to see how model predicts\n",
    "            model=model,\n",
    "            save_dir=save_dir,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        results['augmented_models'][model_name]['graph_metrics'] = graph_metrics\n",
    "    \n",
    "    # Step 8: Save important metrics\n",
    "    if verbose:\n",
    "        print(\"\\n==== Step 8: Saving Important Metrics ====\")\n",
    "    \n",
    "    # Compile accuracy metrics for all models\n",
    "    accuracy_metrics = {\n",
    "        'base_gat': results['base_models']['gat']['metrics'],\n",
    "        'base_sage': results['base_models']['sage']['metrics']\n",
    "    }\n",
    "    \n",
    "    for model_name in augmented_models.keys():\n",
    "        accuracy_metrics[model_name] = augmented_models[model_name]['metrics']\n",
    "    \n",
    "    # Convert to DataFrame and save\n",
    "    accuracy_df = pd.DataFrame.from_dict(accuracy_metrics, orient='index')\n",
    "    accuracy_df.to_csv(os.path.join(comparison_dir, \"model_accuracy_comparison.csv\"))\n",
    "    \n",
    "    # Step 9: Create comprehensive comparison results\n",
    "    if verbose:\n",
    "        print(\"\\n==== Step 9: Creating Comprehensive Comparison ====\")\n",
    "    \n",
    "    # Compare subgraph properties between original and predicted labels\n",
    "    if verbose:\n",
    "        print(\"\\n==== Additional Analysis: Comparing Subgraph Properties ====\")\n",
    "    \n",
    "    # Compare base models' predictions with original properties\n",
    "    base_gat_comparison = compare_subgraph_properties(\n",
    "        original_metrics,\n",
    "        results['base_models']['gat']['graph_metrics'],\n",
    "        os.path.join(comparison_dir, \"base_gat\"),\n",
    "        prefix=\"base_gat\"\n",
    "    )\n",
    "    \n",
    "    base_sage_comparison = compare_subgraph_properties(\n",
    "        original_metrics,\n",
    "        results['base_models']['sage']['graph_metrics'],\n",
    "        os.path.join(comparison_dir, \"base_sage\"),\n",
    "        prefix=\"base_sage\"\n",
    "    )\n",
    "    \n",
    "    # Compare augmented models with original properties\n",
    "    augmented_comparisons = {}\n",
    "    for model_name, model_info in augmented_models.items():\n",
    "        if 'graph_metrics' in model_info:\n",
    "            model_comparison = compare_subgraph_properties(\n",
    "                original_metrics,\n",
    "                model_info['graph_metrics'],\n",
    "                os.path.join(comparison_dir, model_name),\n",
    "                prefix=model_name\n",
    "            )\n",
    "            augmented_comparisons[model_name] = model_comparison\n",
    "    \n",
    "    # Store comparison results\n",
    "    results['comparison']['base_gat'] = base_gat_comparison\n",
    "    results['comparison']['base_sage'] = base_sage_comparison\n",
    "    results['comparison']['augmented'] = augmented_comparisons\n",
    "    \n",
    "    # Compare graph property consistency\n",
    "    all_graph_metrics = {\n",
    "        'original': results['original']['metrics'],\n",
    "        'base_gat': results['base_models']['gat']['graph_metrics'],\n",
    "        'base_sage': results['base_models']['sage']['graph_metrics']\n",
    "    }\n",
    "\n",
    "    for model_name, model_info in augmented_models.items():\n",
    "        if 'graph_metrics' in model_info:\n",
    "            all_graph_metrics[model_name] = model_info['graph_metrics']\n",
    "\n",
    "    # Calculate similarity scores between original and predicted metrics\n",
    "    similarity_scores = {}\n",
    "\n",
    "    for model_name, metrics in all_graph_metrics.items():\n",
    "        if model_name == 'original':\n",
    "            continue\n",
    "            \n",
    "        # Compare with original metrics\n",
    "        similarity = calculate_metrics_similarity(\n",
    "            all_graph_metrics['original'], \n",
    "            metrics,\n",
    "            save_path=os.path.join(comparison_dir, f\"{model_name}_vs_original_similarity.csv\")\n",
    "        )\n",
    "        \n",
    "        similarity_scores[model_name] = similarity\n",
    "\n",
    "    # Create and save similarity comparison plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    models = list(similarity_scores.keys())\n",
    "    scores = [similarity_scores[model]['overall_similarity'] for model in models]\n",
    "\n",
    "    # Use different colors for base models vs augmented models\n",
    "    colors = ['blue' if 'base' in model else 'green' for model in models]\n",
    "\n",
    "    bars = plt.bar(models, scores, color=colors)\n",
    "    plt.title('Graph Property Consistency Scores')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Similarity Score (higher is better)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.4f}', ha='center', va='bottom', rotation=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(comparison_dir, 'graph_property_similarity.png'), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Create summary table with key metrics\n",
    "    summary_metrics = []\n",
    "\n",
    "    # Base models\n",
    "    summary_metrics.append({\n",
    "        'model': 'base_gat',\n",
    "        'test_accuracy': float(results['base_models']['gat']['metrics']['Test Accuracy']),\n",
    "        'training_time': float(results['base_models']['gat']['metrics']['Training Time (s)']),\n",
    "        'consistency_score': similarity_scores['base_gat']['overall_similarity'] if 'base_gat' in similarity_scores else 0\n",
    "    })\n",
    "\n",
    "    summary_metrics.append({\n",
    "        'model': 'base_sage',\n",
    "        'test_accuracy': float(results['base_models']['sage']['metrics']['Test Accuracy']),\n",
    "        'training_time': float(results['base_models']['sage']['metrics']['Training Time (s)']),\n",
    "        'consistency_score': similarity_scores['base_sage']['overall_similarity'] if 'base_sage' in similarity_scores else 0\n",
    "    })\n",
    "\n",
    "    # Augmented models\n",
    "    for model_name, model_info in augmented_models.items():\n",
    "        summary_metrics.append({\n",
    "            'model': model_name,\n",
    "            'test_accuracy': float(model_info['metrics']['Test Accuracy']),\n",
    "            'training_time': float(model_info['metrics']['Training Time (s)']),\n",
    "            'consistency_score': similarity_scores[model_name]['overall_similarity'] if model_name in similarity_scores else 0\n",
    "        })\n",
    "\n",
    "    # Add property preservation scores to the summary metrics\n",
    "    for model_data in summary_metrics:\n",
    "        model_name = model_data['model']\n",
    "        if model_name == 'base_gat' and 'base_gat' in results['comparison']:\n",
    "            model_data['property_preservation'] = results['comparison']['base_gat']['overall_similarity']\n",
    "        elif model_name == 'base_sage' and 'base_sage' in results['comparison']:\n",
    "            model_data['property_preservation'] = results['comparison']['base_sage']['overall_similarity']\n",
    "        elif model_name in augmented_comparisons:\n",
    "            model_data['property_preservation'] = augmented_comparisons[model_name]['overall_similarity']\n",
    "        else:\n",
    "            model_data['property_preservation'] = 0\n",
    "    \n",
    "    # Update combined score to include property preservation\n",
    "    for i, model_data in enumerate(summary_metrics):\n",
    "        summary_metrics[i]['combined_score'] = (\n",
    "            model_data['test_accuracy'] * 0.5 +  # Weight accuracy\n",
    "            model_data['consistency_score'] * 0.2 +  # Weight structure consistency\n",
    "            model_data['property_preservation'] * 0.3  # Weight property preservation\n",
    "        )\n",
    "    \n",
    "    # Update summary DataFrame\n",
    "    summary_df = pd.DataFrame(summary_metrics)\n",
    "    summary_df.to_csv(os.path.join(comparison_dir, 'summary_metrics.csv'), index=False)\n",
    "    \n",
    "    # Find the best model based on updated combined metrics\n",
    "    best_model = summary_df.sort_values('combined_score', ascending=False).iloc[0]\n",
    "    \n",
    "    # Create conclusion to include property preservation\n",
    "    conclusion = f\"\"\"\n",
    "    ===== ANALYSIS CONCLUSION =====\n",
    "    \n",
    "    Best Overall Model: {best_model['model']}\n",
    "    Test Accuracy: {best_model['test_accuracy']:.4f}\n",
    "    Graph Consistency Score: {best_model['consistency_score']:.4f}\n",
    "    Property Preservation Score: {best_model['property_preservation']:.4f}\n",
    "    Combined Score: {best_model['combined_score']:.4f}\n",
    "    Training Time: {best_model['training_time']:.2f} seconds\n",
    "    \n",
    "    Key Findings:\n",
    "    - The best model for test accuracy was: {summary_df.loc[summary_df['test_accuracy'].idxmax(), 'model']} ({summary_df['test_accuracy'].max():.4f})\n",
    "    - The best model for graph consistency was: {summary_df.loc[summary_df['consistency_score'].idxmax(), 'model']} ({summary_df['consistency_score'].max():.4f})\n",
    "    - The best model for preserving subgraph properties was: {summary_df.loc[summary_df['property_preservation'].idxmax(), 'model']} ({summary_df['property_preservation'].max():.4f})\n",
    "    - The fastest model was: {summary_df.loc[summary_df['training_time'].idxmin(), 'model']} ({summary_df['training_time'].min():.2f} seconds)\n",
    "    \n",
    "    Recommendation:\n",
    "    The {best_model['model']} model provides the best balance between prediction accuracy, graph structure preservation, \n",
    "    and maintaining the characteristic subgraph properties of licit and illicit nodes.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(conclusion)\n",
    "    \n",
    "    # Save conclusion\n",
    "    with open(os.path.join(comparison_dir, 'conclusion.txt'), 'w') as f:\n",
    "        f.write(conclusion)\n",
    "    \n",
    "    # Save overall results dictionary (without model objects which aren't serializable)\n",
    "    serializable_results = {}\n",
    "    for key, value in results.items():\n",
    "        if key in ['original', 'comparison']:\n",
    "            serializable_results[key] = value\n",
    "        else:\n",
    "            serializable_results[key] = {}\n",
    "            for subkey, subvalue in value.items():\n",
    "                if isinstance(subvalue, dict) and 'model' in subvalue:\n",
    "                    serializable_results[key][subkey] = {k: v for k, v in subvalue.items() if k != 'model'}\n",
    "                else:\n",
    "                    serializable_results[key][subkey] = subvalue\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def calculate_metrics_similarity(original_metrics, predicted_metrics, \n",
    "                               save_path=None, key_metrics=None):\n",
    "    \"\"\"\n",
    "    Calculate similarity/consistency between original and predicted graph metrics.\n",
    "    \n",
    "    Args:\n",
    "        original_metrics (dict): Original graph metrics\n",
    "        predicted_metrics (dict): Predicted graph metrics\n",
    "        save_path (str, optional): Path to save the comparison\n",
    "        key_metrics (list, optional): List of key metrics to prioritize\n",
    "        \n",
    "    Returns:\n",
    "        dict: Similarity scores\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    # Define key metrics if not provided\n",
    "    if key_metrics is None:\n",
    "        key_metrics = [\n",
    "            'homophily', 'density', 'avg_clustering', 'avg_degree_centrality',\n",
    "            'avg_betweenness_centrality', 'largest_component_ratio'\n",
    "        ]\n",
    "    \n",
    "    # Common groups\n",
    "    common_groups = set(original_metrics.keys()).intersection(set(predicted_metrics.keys()))\n",
    "    \n",
    "    # Calculate similarity for each metric\n",
    "    similarity_scores = {}\n",
    "    all_scores = []\n",
    "    \n",
    "    for group in common_groups:\n",
    "        group_similarity = {}\n",
    "        group_scores = []\n",
    "        \n",
    "        # Check if group exists in both metrics\n",
    "        if group in original_metrics and group in predicted_metrics:\n",
    "            orig_group = original_metrics[group]\n",
    "            pred_group = predicted_metrics[group]\n",
    "            \n",
    "            # Find common metrics\n",
    "            common_metrics = set(orig_group.keys()).intersection(set(pred_group.keys()))\n",
    "            \n",
    "            for metric in common_metrics:\n",
    "                if metric in ['num_nodes', 'num_edges', 'num_components']:\n",
    "                    continue  # Skip count metrics\n",
    "                \n",
    "                orig_val = orig_group.get(metric, 0)\n",
    "                pred_val = pred_group.get(metric, 0)\n",
    "                \n",
    "                # Calculate similarity (1 - relative difference)\n",
    "                if orig_val != 0:\n",
    "                    rel_diff = abs(pred_val - orig_val) / abs(orig_val)\n",
    "                    similarity = max(0, 1 - min(rel_diff, 1))  # Cap at 0-1 range\n",
    "                else:\n",
    "                    # If original is 0, check if prediction is also close to 0\n",
    "                    similarity = 1 if abs(pred_val) < 0.01 else 0\n",
    "                \n",
    "                group_similarity[metric] = similarity\n",
    "                group_scores.append(similarity)\n",
    "                \n",
    "                # If this is a key metric, add it to the overall scores with higher weight\n",
    "                if metric in key_metrics:\n",
    "                    all_scores.append(similarity)\n",
    "                    all_scores.append(similarity)  # Add twice for more weight\n",
    "                else:\n",
    "                    all_scores.append(similarity)\n",
    "        \n",
    "        # Calculate group average\n",
    "        if group_scores:\n",
    "            group_similarity['average'] = np.mean(group_scores)\n",
    "            similarity_scores[group] = group_similarity\n",
    "    \n",
    "    # Calculate overall similarity\n",
    "    overall_similarity = np.mean(all_scores) if all_scores else 0\n",
    "    similarity_scores['overall_similarity'] = overall_similarity\n",
    "    \n",
    "    # Save comparison if path provided\n",
    "    if save_path:\n",
    "        comparison_data = []\n",
    "        \n",
    "        for group in common_groups:\n",
    "            if group in similarity_scores:\n",
    "                for metric, score in similarity_scores[group].items():\n",
    "                    if metric != 'average':\n",
    "                        comparison_data.append({\n",
    "                            'group': group,\n",
    "                            'metric': metric,\n",
    "                            'original_value': original_metrics[group].get(metric, np.nan),\n",
    "                            'predicted_value': predicted_metrics[group].get(metric, np.nan),\n",
    "                            'similarity_score': score\n",
    "                        })\n",
    "        \n",
    "        # Convert to DataFrame and save\n",
    "        df = pd.DataFrame(comparison_data)\n",
    "        df.to_csv(save_path, index=False)\n",
    "    \n",
    "    return similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Step 1: Analyzing Original Graph Properties ====\n",
      "Processed train_licit: 3602 nodes, 651 edges\n",
      "Processed train_illicit: 33649 nodes, 21207 edges\n",
      "Processed val_licit: 508 nodes, 18 edges\n",
      "Processed val_illicit: 4148 nodes, 379 edges\n",
      "Processed test_licit: 435 nodes, 8 edges\n",
      "Processed test_illicit: 4222 nodes, 394 edges\n",
      "Processed unknown: 157205 nodes, 131778 edges\n",
      "Analysis complete. Results saved to output/comprehensive_analysis_20250410_201946/original/graph_properties\n",
      "\n",
      "==== Step 2: Training Base Models ====\n",
      "GAT Epoch: 010, Loss: 0.8470, Train Acc: 0.7809, Val Acc: 0.7788\n",
      "GAT Epoch: 020, Loss: 0.6171, Train Acc: 0.7802, Val Acc: 0.7801\n",
      "GAT Epoch: 030, Loss: 0.4966, Train Acc: 0.7180, Val Acc: 0.7118\n",
      "GAT Epoch: 040, Loss: 0.4165, Train Acc: 0.8239, Val Acc: 0.8232\n",
      "GAT Epoch: 050, Loss: 0.3771, Train Acc: 0.8344, Val Acc: 0.8344\n",
      "GAT Epoch: 060, Loss: 0.3567, Train Acc: 0.8602, Val Acc: 0.8582\n",
      "GAT Epoch: 070, Loss: 0.3427, Train Acc: 0.8573, Val Acc: 0.8567\n",
      "GAT Epoch: 080, Loss: 0.3331, Train Acc: 0.8895, Val Acc: 0.8819\n",
      "GAT Epoch: 090, Loss: 0.3228, Train Acc: 0.9001, Val Acc: 0.8967\n",
      "GAT Epoch: 100, Loss: 0.3116, Train Acc: 0.9023, Val Acc: 0.8978\n",
      "GAT Epoch: 110, Loss: 0.3078, Train Acc: 0.9228, Val Acc: 0.9180\n",
      "GAT Epoch: 120, Loss: 0.2999, Train Acc: 0.9108, Val Acc: 0.9066\n",
      "GAT Epoch: 130, Loss: 0.2946, Train Acc: 0.9235, Val Acc: 0.9162\n",
      "GAT Epoch: 140, Loss: 0.2911, Train Acc: 0.9219, Val Acc: 0.9175\n",
      "GAT Epoch: 150, Loss: 0.2880, Train Acc: 0.9192, Val Acc: 0.9089\n",
      "GAT Epoch: 160, Loss: 0.2866, Train Acc: 0.9261, Val Acc: 0.9186\n",
      "GAT Epoch: 170, Loss: 0.2796, Train Acc: 0.9251, Val Acc: 0.9180\n",
      "GAT Epoch: 180, Loss: 0.2775, Train Acc: 0.9158, Val Acc: 0.9049\n",
      "GAT Epoch: 190, Loss: 0.2774, Train Acc: 0.9181, Val Acc: 0.9070\n",
      "GAT Epoch: 200, Loss: 0.2723, Train Acc: 0.9179, Val Acc: 0.9068\n",
      "GAT Test Accuracy: 0.9216\n",
      "GAT Training Time: 427.67 seconds\n",
      "GraphSAGE Epoch: 010, Loss: 0.3861, Train Acc: 0.9041, Val Acc: 0.8937\n",
      "GraphSAGE Epoch: 020, Loss: 0.2166, Train Acc: 0.9226, Val Acc: 0.9162\n",
      "GraphSAGE Epoch: 030, Loss: 0.1981, Train Acc: 0.9372, Val Acc: 0.9338\n",
      "GraphSAGE Epoch: 040, Loss: 0.1716, Train Acc: 0.9386, Val Acc: 0.9334\n",
      "GraphSAGE Epoch: 050, Loss: 0.1588, Train Acc: 0.9495, Val Acc: 0.9450\n",
      "GraphSAGE Epoch: 060, Loss: 0.1452, Train Acc: 0.9590, Val Acc: 0.9555\n",
      "GraphSAGE Epoch: 070, Loss: 0.1368, Train Acc: 0.9652, Val Acc: 0.9641\n",
      "GraphSAGE Epoch: 080, Loss: 0.1253, Train Acc: 0.9677, Val Acc: 0.9663\n",
      "GraphSAGE Epoch: 090, Loss: 0.1232, Train Acc: 0.9694, Val Acc: 0.9701\n",
      "GraphSAGE Epoch: 100, Loss: 0.1161, Train Acc: 0.9710, Val Acc: 0.9708\n",
      "GraphSAGE Epoch: 110, Loss: 0.1121, Train Acc: 0.9720, Val Acc: 0.9712\n",
      "GraphSAGE Epoch: 120, Loss: 0.1095, Train Acc: 0.9730, Val Acc: 0.9725\n",
      "GraphSAGE Epoch: 130, Loss: 0.1044, Train Acc: 0.9741, Val Acc: 0.9736\n",
      "GraphSAGE Epoch: 140, Loss: 0.1024, Train Acc: 0.9750, Val Acc: 0.9751\n",
      "GraphSAGE Epoch: 150, Loss: 0.0995, Train Acc: 0.9758, Val Acc: 0.9753\n",
      "GraphSAGE Epoch: 160, Loss: 0.0971, Train Acc: 0.9765, Val Acc: 0.9757\n",
      "GraphSAGE Epoch: 170, Loss: 0.0948, Train Acc: 0.9769, Val Acc: 0.9766\n",
      "GraphSAGE Epoch: 180, Loss: 0.0935, Train Acc: 0.9773, Val Acc: 0.9768\n",
      "GraphSAGE Epoch: 190, Loss: 0.0903, Train Acc: 0.9777, Val Acc: 0.9768\n",
      "GraphSAGE Epoch: 200, Loss: 0.0904, Train Acc: 0.9779, Val Acc: 0.9766\n",
      "GraphSAGE Test Accuracy: 0.9740\n",
      "GraphSAGE Training Time: 74.68 seconds\n",
      "\n",
      "==== Step 3: Inspecting Base Model Results ====\n",
      "\n",
      "==================== base_gat Results ====================\n",
      "Model: base_gat\n",
      "Test Accuracy: 0.9216\n",
      "Validation Accuracy: 0.9212\n",
      "Training Time (s): 427.67\n",
      "Final Training Loss: 0.2723\n",
      "Number of Epochs: 200\n",
      "==================================================\n",
      "\n",
      "==================== base_sage Results ====================\n",
      "Model: base_sage\n",
      "Test Accuracy: 0.9740\n",
      "Validation Accuracy: 0.9770\n",
      "Training Time (s): 74.68\n",
      "Final Training Loss: 0.0904\n",
      "Number of Epochs: 200\n",
      "==================================================\n",
      "Processed train_licit: 3602 nodes, 651 edges\n",
      "Processed train_illicit: 33649 nodes, 21207 edges\n",
      "Processed val_licit: 508 nodes, 18 edges\n",
      "Processed val_illicit: 4148 nodes, 379 edges\n",
      "Processed test_licit: 435 nodes, 8 edges\n",
      "Processed test_illicit: 4222 nodes, 394 edges\n",
      "Processed pred_unknown_licit: 2924 nodes, 290 edges\n",
      "Processed pred_unknown_illicit: 154281 nodes, 129899 edges\n",
      "Unknown nodes (157205) predicted as:\n",
      "  - Licit: 2924 (1.86%)\n",
      "  - Illicit: 154281 (98.14%)\n",
      "Analysis complete. Results saved to output/comprehensive_analysis_20250410_201946/models/base_gat/graph_properties\n",
      "Processed train_licit: 3602 nodes, 651 edges\n",
      "Processed train_illicit: 33649 nodes, 21207 edges\n",
      "Processed val_licit: 508 nodes, 18 edges\n",
      "Processed val_illicit: 4148 nodes, 379 edges\n",
      "Processed test_licit: 435 nodes, 8 edges\n",
      "Processed test_illicit: 4222 nodes, 394 edges\n",
      "Processed pred_unknown_licit: 9835 nodes, 1599 edges\n",
      "Processed pred_unknown_illicit: 147370 nodes, 120051 edges\n",
      "Unknown nodes (157205) predicted as:\n",
      "  - Licit: 9835 (6.26%)\n",
      "  - Illicit: 147370 (93.74%)\n",
      "Analysis complete. Results saved to output/comprehensive_analysis_20250410_201946/models/base_sage/graph_properties\n",
      "\n",
      "==== Step 4: Augmenting Data with Different Methods ====\n",
      "Using self-supervised learning for label augmentation\n",
      "Added 47161 previously unknown nodes to training set:\n",
      "  - Predicted illicit: 45741 (96.99%)\n",
      "  - Predicted licit: 1420 (3.01%)\n",
      "Using provided model for label augmentation\n",
      "Added 47161 previously unknown nodes to training set:\n",
      "  - Predicted illicit: 47130 (99.93%)\n",
      "  - Predicted licit: 31 (0.07%)\n",
      "Using provided model for label augmentation\n",
      "Added 47161 previously unknown nodes to training set:\n",
      "  - Predicted illicit: 47135 (99.94%)\n",
      "  - Predicted licit: 26 (0.06%)\n",
      "\n",
      "==== Step 5: Analyzing Augmented Graph Properties ====\n",
      "Processed train_licit: 5022 nodes, 2172 edges\n",
      "Processed train_illicit: 79390 nodes, 77956 edges\n",
      "Processed val_licit: 508 nodes, 18 edges\n",
      "Processed val_illicit: 4148 nodes, 379 edges\n",
      "Processed test_licit: 435 nodes, 8 edges\n",
      "Processed test_illicit: 4222 nodes, 394 edges\n",
      "Processed unknown: 110044 nodes, 86291 edges\n",
      "Analysis complete. Results saved to output/comprehensive_analysis_20250410_201946/augmented/label_propagation/graph_properties\n",
      "Processed train_licit: 3633 nodes, 655 edges\n",
      "Processed train_illicit: 80779 nodes, 79305 edges\n",
      "Processed val_licit: 508 nodes, 18 edges\n",
      "Processed val_illicit: 4148 nodes, 379 edges\n",
      "Processed test_licit: 435 nodes, 8 edges\n",
      "Processed test_illicit: 4222 nodes, 394 edges\n",
      "Processed unknown: 110044 nodes, 82403 edges\n",
      "Analysis complete. Results saved to output/comprehensive_analysis_20250410_201946/augmented/gat/graph_properties\n",
      "Processed train_licit: 3628 nodes, 651 edges\n",
      "Processed train_illicit: 80784 nodes, 71659 edges\n",
      "Processed val_licit: 508 nodes, 18 edges\n",
      "Processed val_illicit: 4148 nodes, 379 edges\n",
      "Processed test_licit: 435 nodes, 8 edges\n",
      "Processed test_illicit: 4222 nodes, 394 edges\n",
      "Processed unknown: 110044 nodes, 73468 edges\n",
      "Analysis complete. Results saved to output/comprehensive_analysis_20250410_201946/augmented/sage/graph_properties\n",
      "\n",
      "==== Step 6: Training Models on Augmented Graphs ====\n",
      "GAT Epoch: 010, Loss: 1.0465, Train Acc: 0.9265, Val Acc: 0.8881\n",
      "GAT Epoch: 020, Loss: 0.7376, Train Acc: 0.5126, Val Acc: 0.5900\n",
      "GAT Epoch: 030, Loss: 0.5714, Train Acc: 0.7066, Val Acc: 0.7496\n",
      "GAT Epoch: 040, Loss: 0.4970, Train Acc: 0.8113, Val Acc: 0.8533\n",
      "GAT Epoch: 050, Loss: 0.4174, Train Acc: 0.9364, Val Acc: 0.9104\n",
      "GAT Epoch: 060, Loss: 0.3802, Train Acc: 0.9063, Val Acc: 0.9008\n",
      "GAT Epoch: 070, Loss: 0.3518, Train Acc: 0.8621, Val Acc: 0.8759\n",
      "GAT Epoch: 080, Loss: 0.3251, Train Acc: 0.8627, Val Acc: 0.8771\n",
      "GAT Epoch: 090, Loss: 0.3102, Train Acc: 0.8492, Val Acc: 0.8711\n",
      "GAT Epoch: 100, Loss: 0.2989, Train Acc: 0.8868, Val Acc: 0.8890\n",
      "GAT Epoch: 110, Loss: 0.2879, Train Acc: 0.9264, Val Acc: 0.9025\n",
      "GAT Epoch: 120, Loss: 0.2781, Train Acc: 0.9374, Val Acc: 0.8973\n",
      "GAT Epoch: 130, Loss: 0.2692, Train Acc: 0.9387, Val Acc: 0.8937\n",
      "GAT Epoch: 140, Loss: 0.2655, Train Acc: 0.9395, Val Acc: 0.8933\n",
      "GAT Epoch: 150, Loss: 0.2574, Train Acc: 0.9397, Val Acc: 0.8920\n",
      "GAT Epoch: 160, Loss: 0.2534, Train Acc: 0.9397, Val Acc: 0.8915\n",
      "GAT Epoch: 170, Loss: 0.2493, Train Acc: 0.9399, Val Acc: 0.8915\n",
      "GAT Epoch: 180, Loss: 0.2454, Train Acc: 0.9402, Val Acc: 0.8911\n",
      "GAT Epoch: 190, Loss: 0.2420, Train Acc: 0.9402, Val Acc: 0.8909\n",
      "GAT Epoch: 200, Loss: 0.2386, Train Acc: 0.9402, Val Acc: 0.8907\n",
      "GAT Test Accuracy: 0.9066\n",
      "GAT Training Time: 412.44 seconds\n",
      "GraphSAGE Epoch: 010, Loss: 0.2371, Train Acc: 0.9354, Val Acc: 0.8997\n",
      "GraphSAGE Epoch: 020, Loss: 0.1762, Train Acc: 0.9417, Val Acc: 0.8939\n",
      "GraphSAGE Epoch: 030, Loss: 0.1570, Train Acc: 0.9432, Val Acc: 0.8969\n",
      "GraphSAGE Epoch: 040, Loss: 0.1467, Train Acc: 0.9482, Val Acc: 0.9130\n",
      "GraphSAGE Epoch: 050, Loss: 0.1404, Train Acc: 0.9512, Val Acc: 0.9182\n",
      "GraphSAGE Epoch: 060, Loss: 0.1366, Train Acc: 0.9549, Val Acc: 0.9272\n",
      "GraphSAGE Epoch: 070, Loss: 0.1325, Train Acc: 0.9574, Val Acc: 0.9334\n",
      "GraphSAGE Epoch: 080, Loss: 0.1286, Train Acc: 0.9600, Val Acc: 0.9403\n",
      "GraphSAGE Epoch: 090, Loss: 0.1250, Train Acc: 0.9616, Val Acc: 0.9461\n",
      "GraphSAGE Epoch: 100, Loss: 0.1235, Train Acc: 0.9629, Val Acc: 0.9489\n",
      "GraphSAGE Epoch: 110, Loss: 0.1210, Train Acc: 0.9641, Val Acc: 0.9517\n",
      "GraphSAGE Epoch: 120, Loss: 0.1196, Train Acc: 0.9649, Val Acc: 0.9536\n",
      "GraphSAGE Epoch: 130, Loss: 0.1171, Train Acc: 0.9658, Val Acc: 0.9585\n",
      "GraphSAGE Epoch: 140, Loss: 0.1152, Train Acc: 0.9663, Val Acc: 0.9598\n",
      "GraphSAGE Epoch: 150, Loss: 0.1140, Train Acc: 0.9666, Val Acc: 0.9594\n",
      "GraphSAGE Epoch: 160, Loss: 0.1132, Train Acc: 0.9672, Val Acc: 0.9594\n",
      "GraphSAGE Epoch: 170, Loss: 0.1124, Train Acc: 0.9675, Val Acc: 0.9605\n",
      "GraphSAGE Epoch: 180, Loss: 0.1115, Train Acc: 0.9679, Val Acc: 0.9618\n",
      "GraphSAGE Epoch: 190, Loss: 0.1100, Train Acc: 0.9682, Val Acc: 0.9620\n",
      "GraphSAGE Epoch: 200, Loss: 0.1096, Train Acc: 0.9684, Val Acc: 0.9620\n",
      "GraphSAGE Test Accuracy: 0.9652\n",
      "GraphSAGE Training Time: 74.20 seconds\n",
      "\n",
      "==================== label_propagation_gat Results ====================\n",
      "Model: label_propagation_gat\n",
      "Test Accuracy: 0.9066\n",
      "Validation Accuracy: 0.9117\n",
      "Training Time (s): 412.44\n",
      "Final Training Loss: 0.2386\n",
      "Number of Epochs: 200\n",
      "==================================================\n",
      "\n",
      "==================== label_propagation_sage Results ====================\n",
      "Model: label_propagation_sage\n",
      "Test Accuracy: 0.9652\n",
      "Validation Accuracy: 0.9637\n",
      "Training Time (s): 74.20\n",
      "Final Training Loss: 0.1096\n",
      "Number of Epochs: 200\n",
      "==================================================\n",
      "GAT Epoch: 010, Loss: 0.8392, Train Acc: 0.9551, Val Acc: 0.8892\n",
      "GAT Epoch: 020, Loss: 0.6842, Train Acc: 0.9541, Val Acc: 0.8937\n",
      "GAT Epoch: 030, Loss: 0.4867, Train Acc: 0.7554, Val Acc: 0.6572\n",
      "GAT Epoch: 040, Loss: 0.4058, Train Acc: 0.8807, Val Acc: 0.7537\n",
      "GAT Epoch: 050, Loss: 0.3535, Train Acc: 0.9004, Val Acc: 0.7792\n",
      "GAT Epoch: 060, Loss: 0.3265, Train Acc: 0.9323, Val Acc: 0.8475\n",
      "GAT Epoch: 070, Loss: 0.2939, Train Acc: 0.9278, Val Acc: 0.8351\n",
      "GAT Epoch: 080, Loss: 0.2736, Train Acc: 0.9319, Val Acc: 0.8406\n",
      "GAT Epoch: 090, Loss: 0.2597, Train Acc: 0.9363, Val Acc: 0.8494\n",
      "GAT Epoch: 100, Loss: 0.2474, Train Acc: 0.9436, Val Acc: 0.8664\n",
      "GAT Epoch: 110, Loss: 0.2343, Train Acc: 0.9477, Val Acc: 0.8726\n",
      "GAT Epoch: 120, Loss: 0.2250, Train Acc: 0.9524, Val Acc: 0.8834\n",
      "GAT Epoch: 130, Loss: 0.2151, Train Acc: 0.9541, Val Acc: 0.8883\n",
      "GAT Epoch: 140, Loss: 0.2106, Train Acc: 0.9570, Val Acc: 0.8943\n",
      "GAT Epoch: 150, Loss: 0.2039, Train Acc: 0.9594, Val Acc: 0.8993\n",
      "GAT Epoch: 160, Loss: 0.1989, Train Acc: 0.9614, Val Acc: 0.9034\n",
      "GAT Epoch: 170, Loss: 0.1941, Train Acc: 0.9626, Val Acc: 0.9025\n",
      "GAT Epoch: 180, Loss: 0.1892, Train Acc: 0.9627, Val Acc: 0.9057\n",
      "GAT Epoch: 190, Loss: 0.1860, Train Acc: 0.9626, Val Acc: 0.9046\n",
      "GAT Epoch: 200, Loss: 0.1818, Train Acc: 0.9624, Val Acc: 0.9038\n",
      "GAT Test Accuracy: 0.9182\n",
      "GAT Training Time: 411.13 seconds\n",
      "GraphSAGE Epoch: 010, Loss: 0.2032, Train Acc: 0.9570, Val Acc: 0.8909\n",
      "GraphSAGE Epoch: 020, Loss: 0.1219, Train Acc: 0.9605, Val Acc: 0.9036\n",
      "GraphSAGE Epoch: 030, Loss: 0.1038, Train Acc: 0.9628, Val Acc: 0.9072\n",
      "GraphSAGE Epoch: 040, Loss: 0.0915, Train Acc: 0.9621, Val Acc: 0.9040\n",
      "GraphSAGE Epoch: 050, Loss: 0.0817, Train Acc: 0.9714, Val Acc: 0.9272\n",
      "GraphSAGE Epoch: 060, Loss: 0.0742, Train Acc: 0.9761, Val Acc: 0.9409\n",
      "GraphSAGE Epoch: 070, Loss: 0.0688, Train Acc: 0.9794, Val Acc: 0.9489\n",
      "GraphSAGE Epoch: 080, Loss: 0.0656, Train Acc: 0.9812, Val Acc: 0.9536\n",
      "GraphSAGE Epoch: 090, Loss: 0.0618, Train Acc: 0.9823, Val Acc: 0.9575\n",
      "GraphSAGE Epoch: 100, Loss: 0.0606, Train Acc: 0.9835, Val Acc: 0.9618\n",
      "GraphSAGE Epoch: 110, Loss: 0.0576, Train Acc: 0.9846, Val Acc: 0.9637\n",
      "GraphSAGE Epoch: 120, Loss: 0.0560, Train Acc: 0.9855, Val Acc: 0.9659\n",
      "GraphSAGE Epoch: 130, Loss: 0.0547, Train Acc: 0.9865, Val Acc: 0.9691\n",
      "GraphSAGE Epoch: 140, Loss: 0.0534, Train Acc: 0.9871, Val Acc: 0.9699\n",
      "GraphSAGE Epoch: 150, Loss: 0.0528, Train Acc: 0.9875, Val Acc: 0.9714\n",
      "GraphSAGE Epoch: 160, Loss: 0.0506, Train Acc: 0.9877, Val Acc: 0.9710\n",
      "GraphSAGE Epoch: 170, Loss: 0.0502, Train Acc: 0.9880, Val Acc: 0.9721\n",
      "GraphSAGE Epoch: 180, Loss: 0.0489, Train Acc: 0.9883, Val Acc: 0.9719\n",
      "GraphSAGE Epoch: 190, Loss: 0.0485, Train Acc: 0.9885, Val Acc: 0.9734\n",
      "GraphSAGE Epoch: 200, Loss: 0.0469, Train Acc: 0.9886, Val Acc: 0.9734\n",
      "GraphSAGE Test Accuracy: 0.9704\n",
      "GraphSAGE Training Time: 73.84 seconds\n",
      "\n",
      "==================== gat_gat Results ====================\n",
      "Model: gat_gat\n",
      "Test Accuracy: 0.9182\n",
      "Validation Accuracy: 0.9064\n",
      "Training Time (s): 411.13\n",
      "Final Training Loss: 0.1818\n",
      "Number of Epochs: 200\n",
      "==================================================\n",
      "\n",
      "==================== gat_sage Results ====================\n",
      "Model: gat_sage\n",
      "Test Accuracy: 0.9704\n",
      "Validation Accuracy: 0.9738\n",
      "Training Time (s): 73.84\n",
      "Final Training Loss: 0.0469\n",
      "Number of Epochs: 200\n",
      "==================================================\n",
      "GAT Epoch: 010, Loss: 0.9191, Train Acc: 0.9559, Val Acc: 0.8902\n",
      "GAT Epoch: 020, Loss: 0.7367, Train Acc: 0.9496, Val Acc: 0.8913\n",
      "GAT Epoch: 030, Loss: 0.5664, Train Acc: 0.8953, Val Acc: 0.8471\n",
      "GAT Epoch: 040, Loss: 0.4552, Train Acc: 0.7629, Val Acc: 0.6871\n",
      "GAT Epoch: 050, Loss: 0.3801, Train Acc: 0.9462, Val Acc: 0.8909\n",
      "GAT Epoch: 060, Loss: 0.3413, Train Acc: 0.9407, Val Acc: 0.8823\n",
      "GAT Epoch: 070, Loss: 0.3120, Train Acc: 0.9572, Val Acc: 0.9036\n",
      "GAT Epoch: 080, Loss: 0.2905, Train Acc: 0.9265, Val Acc: 0.8595\n",
      "GAT Epoch: 090, Loss: 0.2703, Train Acc: 0.9439, Val Acc: 0.8776\n",
      "GAT Epoch: 100, Loss: 0.2577, Train Acc: 0.9408, Val Acc: 0.8746\n",
      "GAT Epoch: 110, Loss: 0.2429, Train Acc: 0.9428, Val Acc: 0.8782\n",
      "GAT Epoch: 120, Loss: 0.2369, Train Acc: 0.9502, Val Acc: 0.8883\n",
      "GAT Epoch: 130, Loss: 0.2248, Train Acc: 0.9537, Val Acc: 0.8902\n",
      "GAT Epoch: 140, Loss: 0.2167, Train Acc: 0.9582, Val Acc: 0.8963\n",
      "GAT Epoch: 150, Loss: 0.2109, Train Acc: 0.9601, Val Acc: 0.9029\n",
      "GAT Epoch: 160, Loss: 0.2059, Train Acc: 0.9610, Val Acc: 0.9027\n",
      "GAT Epoch: 170, Loss: 0.1993, Train Acc: 0.9613, Val Acc: 0.9014\n",
      "GAT Epoch: 180, Loss: 0.1962, Train Acc: 0.9611, Val Acc: 0.9014\n",
      "GAT Epoch: 190, Loss: 0.1908, Train Acc: 0.9609, Val Acc: 0.9001\n",
      "GAT Epoch: 200, Loss: 0.1879, Train Acc: 0.9610, Val Acc: 0.9012\n",
      "GAT Test Accuracy: 0.9160\n",
      "GAT Training Time: 411.06 seconds\n",
      "GraphSAGE Epoch: 010, Loss: 0.2470, Train Acc: 0.9570, Val Acc: 0.8905\n",
      "GraphSAGE Epoch: 020, Loss: 0.1552, Train Acc: 0.9613, Val Acc: 0.9049\n",
      "GraphSAGE Epoch: 030, Loss: 0.1217, Train Acc: 0.9686, Val Acc: 0.9274\n",
      "GraphSAGE Epoch: 040, Loss: 0.0962, Train Acc: 0.9694, Val Acc: 0.9227\n",
      "GraphSAGE Epoch: 050, Loss: 0.0864, Train Acc: 0.9718, Val Acc: 0.9285\n",
      "GraphSAGE Epoch: 060, Loss: 0.0771, Train Acc: 0.9770, Val Acc: 0.9409\n",
      "GraphSAGE Epoch: 070, Loss: 0.0715, Train Acc: 0.9797, Val Acc: 0.9500\n",
      "GraphSAGE Epoch: 080, Loss: 0.0669, Train Acc: 0.9818, Val Acc: 0.9560\n",
      "GraphSAGE Epoch: 090, Loss: 0.0643, Train Acc: 0.9829, Val Acc: 0.9594\n",
      "GraphSAGE Epoch: 100, Loss: 0.0620, Train Acc: 0.9838, Val Acc: 0.9611\n",
      "GraphSAGE Epoch: 110, Loss: 0.0604, Train Acc: 0.9846, Val Acc: 0.9639\n",
      "GraphSAGE Epoch: 120, Loss: 0.0583, Train Acc: 0.9850, Val Acc: 0.9652\n",
      "GraphSAGE Epoch: 130, Loss: 0.0571, Train Acc: 0.9856, Val Acc: 0.9680\n",
      "GraphSAGE Epoch: 140, Loss: 0.0550, Train Acc: 0.9859, Val Acc: 0.9682\n",
      "GraphSAGE Epoch: 150, Loss: 0.0540, Train Acc: 0.9864, Val Acc: 0.9689\n",
      "GraphSAGE Epoch: 160, Loss: 0.0531, Train Acc: 0.9869, Val Acc: 0.9701\n",
      "GraphSAGE Epoch: 170, Loss: 0.0524, Train Acc: 0.9874, Val Acc: 0.9714\n",
      "GraphSAGE Epoch: 180, Loss: 0.0502, Train Acc: 0.9878, Val Acc: 0.9721\n",
      "GraphSAGE Epoch: 190, Loss: 0.0499, Train Acc: 0.9880, Val Acc: 0.9719\n",
      "GraphSAGE Epoch: 200, Loss: 0.0489, Train Acc: 0.9882, Val Acc: 0.9725\n",
      "GraphSAGE Test Accuracy: 0.9704\n",
      "GraphSAGE Training Time: 73.73 seconds\n",
      "\n",
      "==================== sage_gat Results ====================\n",
      "Model: sage_gat\n",
      "Test Accuracy: 0.9160\n",
      "Validation Accuracy: 0.9091\n",
      "Training Time (s): 411.06\n",
      "Final Training Loss: 0.1879\n",
      "Number of Epochs: 200\n",
      "==================================================\n",
      "\n",
      "==================== sage_sage Results ====================\n",
      "Model: sage_sage\n",
      "Test Accuracy: 0.9704\n",
      "Validation Accuracy: 0.9727\n",
      "Training Time (s): 73.73\n",
      "Final Training Loss: 0.0489\n",
      "Number of Epochs: 200\n",
      "==================================================\n",
      "\n",
      "==== Step 7: Analyzing Graph Properties with Augmented Models ====\n",
      "Processed train_licit: 3602 nodes, 651 edges\n",
      "Processed train_illicit: 33649 nodes, 21207 edges\n",
      "Processed val_licit: 508 nodes, 18 edges\n",
      "Processed val_illicit: 4148 nodes, 379 edges\n",
      "Processed test_licit: 435 nodes, 8 edges\n",
      "Processed test_illicit: 4222 nodes, 394 edges\n",
      "Processed pred_unknown_licit: 124 nodes, 0 edges\n",
      "Processed pred_unknown_illicit: 157081 nodes, 131750 edges\n",
      "Unknown nodes (157205) predicted as:\n",
      "  - Licit: 124 (0.08%)\n",
      "  - Illicit: 157081 (99.92%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuhuizhan/BTCGraphGuard/venv/lib/python3.10/site-packages/pandas/plotting/_matplotlib/core.py:580: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig = self.plt.figure(figsize=self.figsize)\n",
      "/var/folders/m4/lr92jqdj1_z4cm26h66ndqy40000gn/T/ipykernel_80047/3274953820.py:269: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure(figsize=(12, 8))\n",
      "/var/folders/m4/lr92jqdj1_z4cm26h66ndqy40000gn/T/ipykernel_80047/3274953820.py:322: UserWarning: Dataset has 0 variance; skipping density estimate. Pass `warn_singular=False` to disable this warning.\n",
      "  sns.kdeplot(group_data['degree_centrality'], label=group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete. Results saved to output/comprehensive_analysis_20250410_201946/augmented/label/propagation/graph_properties_with_model\n",
      "Processed train_licit: 3602 nodes, 651 edges\n",
      "Processed train_illicit: 33649 nodes, 21207 edges\n",
      "Processed val_licit: 508 nodes, 18 edges\n",
      "Processed val_illicit: 4148 nodes, 379 edges\n",
      "Processed test_licit: 435 nodes, 8 edges\n",
      "Processed test_illicit: 4222 nodes, 394 edges\n",
      "Processed pred_unknown_licit: 4989 nodes, 1299 edges\n",
      "Processed pred_unknown_illicit: 152216 nodes, 124705 edges\n",
      "Unknown nodes (157205) predicted as:\n",
      "  - Licit: 4989 (3.17%)\n",
      "  - Illicit: 152216 (96.83%)\n",
      "Analysis complete. Results saved to output/comprehensive_analysis_20250410_201946/augmented/label/propagation/graph_properties_with_model\n",
      "Processed train_licit: 3602 nodes, 651 edges\n",
      "Processed train_illicit: 33649 nodes, 21207 edges\n",
      "Processed val_licit: 508 nodes, 18 edges\n",
      "Processed val_illicit: 4148 nodes, 379 edges\n",
      "Processed test_licit: 435 nodes, 8 edges\n",
      "Processed test_illicit: 4222 nodes, 394 edges\n",
      "Processed pred_unknown_licit: 5862 nodes, 1459 edges\n",
      "Processed pred_unknown_illicit: 151343 nodes, 127114 edges\n",
      "Unknown nodes (157205) predicted as:\n",
      "  - Licit: 5862 (3.73%)\n",
      "  - Illicit: 151343 (96.27%)\n",
      "Analysis complete. Results saved to output/comprehensive_analysis_20250410_201946/augmented/gat/gat/graph_properties_with_model\n",
      "Processed train_licit: 3602 nodes, 651 edges\n",
      "Processed train_illicit: 33649 nodes, 21207 edges\n",
      "Processed val_licit: 508 nodes, 18 edges\n",
      "Processed val_illicit: 4148 nodes, 379 edges\n",
      "Processed test_licit: 435 nodes, 8 edges\n",
      "Processed test_illicit: 4222 nodes, 394 edges\n",
      "Processed pred_unknown_licit: 8038 nodes, 1108 edges\n",
      "Processed pred_unknown_illicit: 149167 nodes, 121987 edges\n",
      "Unknown nodes (157205) predicted as:\n",
      "  - Licit: 8038 (5.11%)\n",
      "  - Illicit: 149167 (94.89%)\n",
      "Analysis complete. Results saved to output/comprehensive_analysis_20250410_201946/augmented/gat/sage/graph_properties_with_model\n",
      "Processed train_licit: 3602 nodes, 651 edges\n",
      "Processed train_illicit: 33649 nodes, 21207 edges\n",
      "Processed val_licit: 508 nodes, 18 edges\n",
      "Processed val_illicit: 4148 nodes, 379 edges\n",
      "Processed test_licit: 435 nodes, 8 edges\n",
      "Processed test_illicit: 4222 nodes, 394 edges\n",
      "Processed pred_unknown_licit: 3207 nodes, 433 edges\n",
      "Processed pred_unknown_illicit: 153998 nodes, 129682 edges\n",
      "Unknown nodes (157205) predicted as:\n",
      "  - Licit: 3207 (2.04%)\n",
      "  - Illicit: 153998 (97.96%)\n",
      "Analysis complete. Results saved to output/comprehensive_analysis_20250410_201946/augmented/sage/gat/graph_properties_with_model\n",
      "Processed train_licit: 3602 nodes, 651 edges\n",
      "Processed train_illicit: 33649 nodes, 21207 edges\n",
      "Processed val_licit: 508 nodes, 18 edges\n",
      "Processed val_illicit: 4148 nodes, 379 edges\n",
      "Processed test_licit: 435 nodes, 8 edges\n",
      "Processed test_illicit: 4222 nodes, 394 edges\n",
      "Processed pred_unknown_licit: 8365 nodes, 1002 edges\n",
      "Processed pred_unknown_illicit: 148840 nodes, 121404 edges\n",
      "Unknown nodes (157205) predicted as:\n",
      "  - Licit: 8365 (5.32%)\n",
      "  - Illicit: 148840 (94.68%)\n",
      "Analysis complete. Results saved to output/comprehensive_analysis_20250410_201946/augmented/sage/sage/graph_properties_with_model\n",
      "\n",
      "==== Step 8: Saving Important Metrics ====\n",
      "\n",
      "==== Step 9: Creating Comprehensive Comparison ====\n",
      "\n",
      "==== Additional Analysis: Comparing Subgraph Properties ====\n",
      "\n",
      "    ===== ANALYSIS CONCLUSION =====\n",
      "    \n",
      "    Best Overall Model: base_sage\n",
      "    Test Accuracy: 0.9740\n",
      "    Graph Consistency Score: 1.0000\n",
      "    Property Preservation Score: 0.4917\n",
      "    Combined Score: 0.8345\n",
      "    Training Time: 74.68 seconds\n",
      "    \n",
      "    Key Findings:\n",
      "    - The best model for test accuracy was: base_sage (0.9740)\n",
      "    - The best model for graph consistency was: base_gat (1.0000)\n",
      "    - The best model for preserving subgraph properties was: base_sage (0.4917)\n",
      "    - The fastest model was: sage_sage (73.73 seconds)\n",
      "    \n",
      "    Recommendation:\n",
      "    The base_sage model provides the best balance between prediction accuracy, graph structure preservation, \n",
      "    and maintaining the characteristic subgraph properties of licit and illicit nodes.\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = comprehensive_graph_analysis(data, epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post processing with the saved results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_metrics = [\n",
    "            'homophily', 'density', 'avg_clustering', 'avg_degree_centrality',\n",
    "            'avg_betweenness_centrality', 'largest_component_ratio'\n",
    "        ]\n",
    "\n",
    "def post_processing(csv_path):\n",
    "    \"\"\"\n",
    "    Post-processes graph metrics to compare similarity across different data splits.\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Path to the graph_metrics.csv file\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Comparison DataFrame and overall similarity scores\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import os\n",
    "    \n",
    "    # Define key metrics to analyze\n",
    "    key_metrics = [\n",
    "        'homophily', 'density', 'avg_clustering', 'avg_degree_centrality',\n",
    "        'avg_betweenness_centrality', 'largest_component_ratio'\n",
    "    ]\n",
    "    \n",
    "    # Read the CSV file\n",
    "    metrics_df = pd.read_csv(csv_path, index_col=0)\n",
    "    \n",
    "    # Define the comparison sets\n",
    "    comparison_sets = {\n",
    "        'licit': {\n",
    "            'reference': 'train_licit',\n",
    "            'compare_to': ['val_licit', 'test_licit', 'pred_unknown_licit']\n",
    "        },\n",
    "        'illicit': {\n",
    "            'reference': 'train_illicit',\n",
    "            'compare_to': ['val_illicit', 'test_illicit', 'pred_unknown_illicit']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Function to calculate similarity (1 - normalized difference)\n",
    "    def calculate_similarity(val1, val2):\n",
    "        if val1 == 0 and val2 == 0:\n",
    "            return 1.0  # Both zero means they're identical\n",
    "        elif val1 == 0 or val2 == 0:\n",
    "            return 0.0  # One zero and one non-zero means no similarity\n",
    "        else:\n",
    "            # Calculate similarity as 1 - normalized difference\n",
    "            diff = abs(val1 - val2) / max(abs(val1), abs(val2))\n",
    "            return max(0, 1 - min(diff, 1))  # Cap between 0 and 1\n",
    "    \n",
    "    # Prepare data structure for results\n",
    "    comparison_results = []\n",
    "    \n",
    "    # Calculate similarities\n",
    "    for label_type, sets in comparison_sets.items():\n",
    "        reference_set = sets['reference']\n",
    "        \n",
    "        if reference_set not in metrics_df.index:\n",
    "            print(f\"Warning: Reference set '{reference_set}' not found in the data\")\n",
    "            continue\n",
    "            \n",
    "        for compare_set in sets['compare_to']:\n",
    "            if compare_set not in metrics_df.index:\n",
    "                print(f\"Warning: Comparison set '{compare_set}' not found in the data\")\n",
    "                continue\n",
    "                \n",
    "            # Calculate similarity for each key metric\n",
    "            for metric in key_metrics:\n",
    "                if metric not in metrics_df.columns:\n",
    "                    print(f\"Warning: Metric '{metric}' not found in the data\")\n",
    "                    continue\n",
    "                    \n",
    "                ref_value = metrics_df.loc[reference_set, metric]\n",
    "                comp_value = metrics_df.loc[compare_set, metric]\n",
    "                \n",
    "                similarity = calculate_similarity(ref_value, comp_value)\n",
    "                \n",
    "                comparison_results.append({\n",
    "                    'label_type': label_type,\n",
    "                    'reference_set': reference_set,\n",
    "                    'comparison_set': compare_set,\n",
    "                    'metric': metric,\n",
    "                    'reference_value': ref_value,\n",
    "                    'comparison_value': comp_value,\n",
    "                    'similarity': similarity\n",
    "                })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    comparison_df = pd.DataFrame(comparison_results)\n",
    "    \n",
    "    # Calculate overall similarity scores\n",
    "    overall_scores = comparison_df.groupby(['label_type', 'comparison_set'])['similarity'].mean().reset_index()\n",
    "    overall_scores.columns = ['label_type', 'comparison_set', 'overall_similarity']\n",
    "    \n",
    "    # Save comparison to CSV\n",
    "    output_dir = os.path.dirname(csv_path)\n",
    "    comparison_df.to_csv(os.path.join(output_dir, 'graph_metrics_comparison.csv'), index=False)\n",
    "    overall_scores.to_csv(os.path.join(output_dir, 'graph_metrics_overall_similarity.csv'), index=False)\n",
    "    \n",
    "    # Create visualizations\n",
    "    \n",
    "    # 1. Bar chart of similarities by metric\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.barplot(x='metric', y='similarity', hue='comparison_set', \n",
    "                data=comparison_df, palette='viridis')\n",
    "    plt.title('Similarity to Training Set by Metric')\n",
    "    plt.xlabel('Metric')\n",
    "    plt.ylabel('Similarity Score (higher is better)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Comparison Set')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'metric_similarities_bar.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Heatmap of similarities across metrics and comparison sets\n",
    "    heatmap_data = comparison_df.pivot_table(\n",
    "        index='comparison_set', \n",
    "        columns='metric', \n",
    "        values='similarity', \n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(heatmap_data, annot=True, cmap='YlGnBu', vmin=0, vmax=1, \n",
    "                linewidths=0.5, fmt='.3f')\n",
    "    plt.title('Similarity Heatmap (Comparison to Training Set)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'similarity_heatmap.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Separate plots for licit and illicit\n",
    "    for label_type in comparison_sets.keys():\n",
    "        label_data = comparison_df[comparison_df['label_type'] == label_type]\n",
    "        \n",
    "        if len(label_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Bar chart by comparison set\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x='comparison_set', y='similarity', hue='metric', \n",
    "                    data=label_data, palette='Set2')\n",
    "        plt.title(f'{label_type.title()} Nodes: Similarity to Training Set')\n",
    "        plt.xlabel('Comparison Set')\n",
    "        plt.ylabel('Similarity Score')\n",
    "        plt.legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'{label_type}_similarities_by_set.png'), dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Radar chart\n",
    "        radar_data = label_data.pivot_table(\n",
    "            index='comparison_set', \n",
    "            columns='metric', \n",
    "            values='similarity'\n",
    "        )\n",
    "        \n",
    "        # Create radar chart\n",
    "        fig = plt.figure(figsize=(10, 8))\n",
    "        ax = fig.add_subplot(111, polar=True)\n",
    "        \n",
    "        # Number of metrics\n",
    "        N = len(key_metrics)\n",
    "        \n",
    "        # Angle of each axis\n",
    "        angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "        angles += angles[:1]  # Close the loop\n",
    "        \n",
    "        # Initialize the plot\n",
    "        ax.set_theta_offset(np.pi / 2)\n",
    "        ax.set_theta_direction(-1)\n",
    "        ax.set_rlabel_position(0)\n",
    "        plt.xticks(angles[:-1], key_metrics, size=12)\n",
    "        plt.yticks([0.2, 0.4, 0.6, 0.8, 1.0], [\"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"], size=10)\n",
    "        plt.ylim(0, 1)\n",
    "        \n",
    "        # Plot each comparison set\n",
    "        for i, comparison_set in enumerate(radar_data.index):\n",
    "            values = radar_data.loc[comparison_set].values.flatten().tolist()\n",
    "            values += values[:1]  # Close the loop\n",
    "            \n",
    "            ax.plot(angles, values, linewidth=2, linestyle='solid', \n",
    "                    label=comparison_set)\n",
    "            ax.fill(angles, values, alpha=0.1)\n",
    "        \n",
    "        plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "        plt.title(f'{label_type.title()} Nodes: Metric Similarity Radar Chart', size=15)\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'{label_type}_radar_chart.png'), dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # 4. Overall similarity comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='comparison_set', y='overall_similarity', hue='label_type', \n",
    "                data=overall_scores, palette='Set1')\n",
    "    plt.title('Overall Similarity to Training Set')\n",
    "    plt.xlabel('Comparison Set')\n",
    "    plt.ylabel('Overall Similarity Score')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(title='Label Type')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(plt.gca().patches):\n",
    "        plt.gca().text(\n",
    "            bar.get_x() + bar.get_width()/2, \n",
    "            bar.get_height() + 0.01, \n",
    "            f'{bar.get_height():.3f}', \n",
    "            ha='center', va='bottom'\n",
    "        )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'overall_similarity.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Create a summary table visualization\n",
    "    summary_table = overall_scores.pivot(index='comparison_set', columns='label_type', values='overall_similarity')\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    ax = plt.subplot(111, frame_on=False)\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.yaxis.set_visible(False)\n",
    "    \n",
    "    table = plt.table(\n",
    "        cellText=np.round(summary_table.values, 3),\n",
    "        rowLabels=summary_table.index,\n",
    "        colLabels=summary_table.columns,\n",
    "        cellLoc='center',\n",
    "        loc='center',\n",
    "        cellColours=plt.cm.YlGnBu(summary_table.values/summary_table.values.max())\n",
    "    )\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "    table.scale(1.2, 1.8)\n",
    "    \n",
    "    plt.suptitle('Overall Similarity Scores Summary', fontsize=16, y=0.95)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'similarity_summary_table.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Post-processing complete. Results saved to {output_dir}\")\n",
    "    \n",
    "    return comparison_df, overall_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing(root_folder):\n",
    "    \"\"\"\n",
    "    Post-processes graph metrics across multiple model outputs to compare similarity.\n",
    "    Uses constant original metrics for train/val/test sets.\n",
    "    \n",
    "    Args:\n",
    "        root_folder (str): Path to the root folder containing model outputs\n",
    "    \n",
    "    Returns:\n",
    "        dict: Compiled results showing similarity metrics across methods\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import os\n",
    "    import glob\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # First, let's check the directory structure to understand what we're working with\n",
    "    analysis_dir = os.path.join(root_folder, 'output/comprehensive_analysis_20250410_201946')\n",
    "    print(f\"Analyzing directory structure in: {analysis_dir}\")\n",
    "    \n",
    "    # Check base directories\n",
    "    base_dirs = [d for d in os.listdir(analysis_dir) \n",
    "                 if os.path.isdir(os.path.join(analysis_dir, d)) and d != 'original']\n",
    "    print(f\"Found base directories: {base_dirs}\")\n",
    "    \n",
    "    # Define the target method paths explicitly\n",
    "    target_methods = [\n",
    "        'models/base_gat',\n",
    "        'models/base_sage',\n",
    "        'augmented/label_propagation/gat',\n",
    "        'augmented/label_propagation/sage',\n",
    "        'augmented/gat/gat',\n",
    "        'augmented/gat/sage',\n",
    "        'augmented/sage/gat',\n",
    "        'augmented/sage/sage'\n",
    "    ]\n",
    "    \n",
    "    # Define key metrics to analyze\n",
    "    key_metrics = [\n",
    "        'homophily', 'density', 'avg_clustering', 'avg_degree_centrality',\n",
    "        'avg_betweenness_centrality', 'largest_component_ratio'\n",
    "    ]\n",
    "    \n",
    "    # Load the original metrics file for reference values\n",
    "    original_metrics_path = os.path.join(analysis_dir, 'original/graph_properties/graph_metrics.csv')\n",
    "    \n",
    "    try:\n",
    "        original_metrics_df = pd.read_csv(original_metrics_path, index_col=0)\n",
    "        print(f\"Loaded original metrics from {original_metrics_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading original metrics: {e}\")\n",
    "        print(\"Will proceed with per-method metrics for train/val/test (not recommended)\")\n",
    "        original_metrics_df = None\n",
    "    \n",
    "    # Results storage\n",
    "    method_metrics = {}\n",
    "    normalized_results = {'licit': {}, 'illicit': {}}\n",
    "    \n",
    "    # Check each target method explicitly\n",
    "    for method_path in target_methods:\n",
    "        full_path = os.path.join(analysis_dir, method_path)\n",
    "        \n",
    "        # Define possible metrics paths:\n",
    "        graph_props_path = os.path.join(full_path, 'graph_properties/graph_metrics.csv')\n",
    "        graph_props_with_model_path = os.path.join(full_path, 'graph_properties_with_model/graph_metrics.csv')\n",
    "        \n",
    "        metrics_file = None\n",
    "        if os.path.exists(graph_props_path):\n",
    "            metrics_file = graph_props_path\n",
    "            print(f\"Found graph metrics for {method_path} at: {graph_props_path}\")\n",
    "        elif os.path.exists(graph_props_with_model_path):\n",
    "            metrics_file = graph_props_with_model_path\n",
    "            print(f\"Found graph metrics for {method_path} at: {graph_props_with_model_path}\")\n",
    "        else:\n",
    "            print(f\"No metrics file found for {method_path}\")\n",
    "            continue\n",
    "            \n",
    "        # Create method name from path\n",
    "        if method_path.startswith('models/base_'):\n",
    "            method_name = method_path.split('/')[-1]\n",
    "        else:\n",
    "            parts = method_path.split('/')\n",
    "            method_name = f\"{parts[1]}_{parts[2]}\"\n",
    "        \n",
    "        print(f\"Processing metrics for: {method_name}\")\n",
    "        \n",
    "        # Load the metrics file\n",
    "        try:\n",
    "            metrics_df = pd.read_csv(metrics_file, index_col=0)\n",
    "            \n",
    "            # Check if pred_unknown entries exist\n",
    "            has_pred_unknown_licit = 'pred_unknown_licit' in metrics_df.index\n",
    "            has_pred_unknown_illicit = 'pred_unknown_illicit' in metrics_df.index\n",
    "            \n",
    "            if not has_pred_unknown_licit:\n",
    "                print(f\"Warning: pred_unknown_licit not found in {method_name}\")\n",
    "            if not has_pred_unknown_illicit:\n",
    "                print(f\"Warning: pred_unknown_illicit not found in {method_name}\")\n",
    "                \n",
    "            if not (has_pred_unknown_licit or has_pred_unknown_illicit):\n",
    "                print(f\"Skipping {method_name} as it has no pred_unknown entries\")\n",
    "                continue\n",
    "                \n",
    "            # Store the metrics\n",
    "            method_metrics[method_name] = metrics_df\n",
    "            \n",
    "            # Calculate normalized similarity\n",
    "            calculate_normalized_similarity(\n",
    "                method_name, \n",
    "                metrics_df, \n",
    "                original_metrics_df, \n",
    "                normalized_results,\n",
    "                key_metrics\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {method_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Generate comparison visualizations\n",
    "    comparison_dir = os.path.join(analysis_dir, 'method_comparisons')\n",
    "    os.makedirs(comparison_dir, exist_ok=True)\n",
    "    \n",
    "    create_comparison_visualizations(normalized_results, analysis_dir)\n",
    "    \n",
    "    return normalized_results\n",
    "\n",
    "def calculate_normalized_similarity(method_name, method_df, original_df, normalized_results, key_metrics):\n",
    "    \"\"\"\n",
    "    Calculate normalized similarity between train/val/test and pred_unknown.\n",
    "    \n",
    "    Args:\n",
    "        method_name (str): Name of the method\n",
    "        method_df (DataFrame): Metrics dataframe for this method\n",
    "        original_df (DataFrame): Original metrics for reference\n",
    "        normalized_results (dict): Dictionary to store normalized results\n",
    "        key_metrics (list): List of metrics to analyze\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Calculate similarity between train and val/test sets using original metrics\n",
    "    def calculate_similarity(val1, val2):\n",
    "        if val1 == 0 and val2 == 0:\n",
    "            return 1.0  # Both zero means they're identical\n",
    "        elif val1 == 0 or val2 == 0:\n",
    "            return 0.0  # One zero and one non-zero means no similarity\n",
    "        else:\n",
    "            # Calculate similarity as 1 - normalized difference\n",
    "            diff = abs(val1 - val2) / max(abs(val1), abs(val2))\n",
    "            return max(0, 1 - min(diff, 1))  # Cap between 0 and 1\n",
    "    \n",
    "    # Calculate train->val and train->test similarities for both licit and illicit\n",
    "    train_val_similarities = {'licit': [], 'illicit': []}\n",
    "    train_test_similarities = {'licit': [], 'illicit': []}\n",
    "    \n",
    "    for label_type in ['licit', 'illicit']:\n",
    "        train_key = f'train_{label_type}'\n",
    "        val_key = f'val_{label_type}'\n",
    "        test_key = f'test_{label_type}'\n",
    "        pred_key = f'pred_unknown_{label_type}'\n",
    "        \n",
    "        # Skip if pred_unknown is not in the method's metrics\n",
    "        if pred_key not in method_df.index:\n",
    "            print(f\"Warning: {pred_key} not found in {method_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Use original metrics for train/val/test\n",
    "        for metric in key_metrics:\n",
    "            if metric not in original_df.columns:\n",
    "                continue\n",
    "                \n",
    "            # Calculate train->val similarity using original metrics\n",
    "            train_val_sim = calculate_similarity(\n",
    "                original_df.loc[train_key, metric], \n",
    "                original_df.loc[val_key, metric]\n",
    "            )\n",
    "            train_val_similarities[label_type].append(train_val_sim)\n",
    "            \n",
    "            # Calculate train->test similarity using original metrics\n",
    "            train_test_sim = calculate_similarity(\n",
    "                original_df.loc[train_key, metric], \n",
    "                original_df.loc[test_key, metric]\n",
    "            )\n",
    "            train_test_similarities[label_type].append(train_test_sim)\n",
    "        \n",
    "        # Calculate train->pred similarity using original train and method's pred\n",
    "        train_pred_similarities = []\n",
    "        for metric in key_metrics:\n",
    "            if metric not in method_df.columns:\n",
    "                continue\n",
    "                \n",
    "            train_pred_sim = calculate_similarity(\n",
    "                original_df.loc[train_key, metric], \n",
    "                method_df.loc[pred_key, metric]\n",
    "            )\n",
    "            train_pred_similarities.append(train_pred_sim)\n",
    "        \n",
    "        # Calculate average similarities\n",
    "        avg_train_val = np.mean(train_val_similarities[label_type])\n",
    "        avg_train_test = np.mean(train_test_similarities[label_type])\n",
    "        avg_train_pred = np.mean(train_pred_similarities)\n",
    "        \n",
    "        # Calculate normalized similarity score\n",
    "        # We want to know how the pred->train relationship compares to the val->test relationship\n",
    "        # Ideally, this would be 1.0 (meaning pred->train mimics val->test perfectly)\n",
    "        if avg_train_test == 0:\n",
    "            # Handle division by zero\n",
    "            normalized_sim = float('inf') if avg_train_pred > 0 else 0\n",
    "        else:\n",
    "            # Calculate how much the pred similarity resembles the test similarity\n",
    "            normalized_sim = avg_train_pred / (avg_train_test - avg_train_val)\n",
    "        \n",
    "        # Store the normalized similarity\n",
    "        normalized_results[label_type][method_name] = normalized_sim\n",
    "        print(f\"{method_name} {label_type} normalized similarity: {normalized_sim:.4f}\")\n",
    "\n",
    "def create_comparison_visualizations(normalized_results, output_dir):\n",
    "    \"\"\"\n",
    "    Create visualizations comparing normalized similarity across methods.\n",
    "    \n",
    "    Args:\n",
    "        normalized_results (dict): Dictionary of normalized similarity scores\n",
    "        output_dir (str): Directory to save visualizations\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import numpy as np\n",
    "    \n",
    "    comparison_dir = os.path.join(output_dir, 'method_comparisons')\n",
    "    os.makedirs(comparison_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert results to DataFrames for plotting\n",
    "    licit_data = []\n",
    "    illicit_data = []\n",
    "    \n",
    "    for method, score in normalized_results['licit'].items():\n",
    "        licit_data.append({'Method': method, 'Normalized Similarity': score})\n",
    "    \n",
    "    for method, score in normalized_results['illicit'].items():\n",
    "        illicit_data.append({'Method': method, 'Normalized Similarity': score})\n",
    "    \n",
    "    licit_df = pd.DataFrame(licit_data)\n",
    "    illicit_df = pd.DataFrame(illicit_data)\n",
    "    \n",
    "    # Calculate distance from ideal (1.0)\n",
    "    licit_df['Distance_From_Ideal'] = abs(licit_df['Normalized Similarity'] - 1.0)\n",
    "    illicit_df['Distance_From_Ideal'] = abs(illicit_df['Normalized Similarity'] - 1.0)\n",
    "    \n",
    "    # Sort by distance from ideal (closest first)\n",
    "    licit_df = licit_df.sort_values('Distance_From_Ideal')\n",
    "    illicit_df = illicit_df.sort_values('Distance_From_Ideal')\n",
    "    \n",
    "    # Print top and bottom 5 methods\n",
    "    print(\"\\n===== LICIT NODES: TOP 5 METHODS CLOSEST TO IDEAL SIMILARITY (1.0) =====\")\n",
    "    for _, row in licit_df.head(5).iterrows():\n",
    "        print(f\"{row['Method']}: {row['Normalized Similarity']:.4f} (distance from ideal: {row['Distance_From_Ideal']:.4f})\")\n",
    "    \n",
    "    print(\"\\n===== LICIT NODES: BOTTOM 5 METHODS FURTHEST FROM IDEAL SIMILARITY (1.0) =====\")\n",
    "    for _, row in licit_df.tail(5).iterrows():\n",
    "        print(f\"{row['Method']}: {row['Normalized Similarity']:.4f} (distance from ideal: {row['Distance_From_Ideal']:.4f})\")\n",
    "    \n",
    "    print(\"\\n===== ILLICIT NODES: TOP 5 METHODS CLOSEST TO IDEAL SIMILARITY (1.0) =====\")\n",
    "    for _, row in illicit_df.head(5).iterrows():\n",
    "        print(f\"{row['Method']}: {row['Normalized Similarity']:.4f} (distance from ideal: {row['Distance_From_Ideal']:.4f})\")\n",
    "    \n",
    "    print(\"\\n===== ILLICIT NODES: BOTTOM 5 METHODS FURTHEST FROM IDEAL SIMILARITY (1.0) =====\")\n",
    "    for _, row in illicit_df.tail(5).iterrows():\n",
    "        print(f\"{row['Method']}: {row['Normalized Similarity']:.4f} (distance from ideal: {row['Distance_From_Ideal']:.4f})\")\n",
    "    \n",
    "    # Create visualizations for licit and illicit metrics\n",
    "    create_metric_visualizations(licit_df, 'licit', comparison_dir)\n",
    "    create_metric_visualizations(illicit_df, 'illicit', comparison_dir)\n",
    "    \n",
    "    # Create combined visualization\n",
    "    create_combined_visualization(licit_df, illicit_df, comparison_dir)\n",
    "    \n",
    "    print(f\"Visualizations saved to {comparison_dir}\")\n",
    "\n",
    "def create_metric_visualizations(df, label_type, output_dir):\n",
    "    \"\"\"Create visualizations for a specific label type\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Create horizontal bar chart - show closest 15 methods for clarity\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    top_df = df.head(min(15, len(df)))\n",
    "    colors = ['green' if abs(x-1.0) < 0.1 else 'skyblue' if label_type == 'licit' else 'salmon' \n",
    "              for x in top_df['Normalized Similarity']]\n",
    "    \n",
    "    bars = plt.barh(top_df['Method'], top_df['Normalized Similarity'], color=colors)\n",
    "    plt.title(f'Top Methods with Normalized Similarity Closest to Ideal for {label_type.title()} Nodes')\n",
    "    plt.xlabel('Normalized Similarity Score (closer to 1 is better)')\n",
    "    plt.ylabel('Method')\n",
    "    plt.axvline(x=1.0, color='red', linestyle='--', label='Ideal Value')\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # Add distance labels\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        dist = abs(width - 1.0)\n",
    "        plt.text(width + 0.05, bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.2f} (dist: {dist:.2f})', ha='left', va='center')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'{label_type}_normalized_similarity_top15.png'), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def create_combined_visualization(licit_df, illicit_df, output_dir):\n",
    "    \"\"\"Create a combined visualization of licit and illicit metrics\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Merge the dataframes\n",
    "    merged_df = pd.merge(licit_df, illicit_df, on='Method', suffixes=('_licit', '_illicit'), how='outer')\n",
    "    \n",
    "    # Calculate combined distance (average of licit and illicit distances)\n",
    "    merged_df['Combined_Distance'] = merged_df[['Distance_From_Ideal_licit', 'Distance_From_Ideal_illicit']].mean(axis=1, skipna=True)\n",
    "    \n",
    "    # Sort by combined distance\n",
    "    merged_df = merged_df.sort_values('Combined_Distance')\n",
    "    \n",
    "    # Take top 15 methods\n",
    "    top_df = merged_df.head(min(15, len(merged_df)))\n",
    "    \n",
    "    # Create a summary table CSV\n",
    "    top_df.to_csv(os.path.join(output_dir, 'normalized_similarity_summary.csv'), index=False)\n",
    "    \n",
    "    # Plot comparisons\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    x = range(len(top_df))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Get licit and illicit scores\n",
    "    licit_scores = top_df['Normalized Similarity_licit'].fillna(0)\n",
    "    illicit_scores = top_df['Normalized Similarity_illicit'].fillna(0)\n",
    "    methods = top_df['Method']\n",
    "    \n",
    "    # Derive colors based on distance from ideal\n",
    "    licit_colors = ['green' if abs(score-1.0) < 0.1 else 'skyblue' for score in licit_scores]\n",
    "    illicit_colors = ['green' if abs(score-1.0) < 0.1 else 'salmon' for score in illicit_scores]\n",
    "    \n",
    "    # Plot bars\n",
    "    plt.bar([i - width/2 for i in x], licit_scores, width, label='Licit Score', color=licit_colors)\n",
    "    plt.bar([i + width/2 for i in x], illicit_scores, width, label='Illicit Score', color=illicit_colors)\n",
    "    \n",
    "    plt.axhline(y=1.0, color='red', linestyle='--', label='Ideal Value')\n",
    "    plt.xlabel('Method')\n",
    "    plt.ylabel('Normalized Similarity Score')\n",
    "    plt.title('Top Methods by Proximity to Ideal Similarity Score (1.0)')\n",
    "    plt.xticks(x, methods, rotation=90)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # Add combined distance labels\n",
    "    for i, method in enumerate(methods):\n",
    "        combined_dist = top_df.loc[top_df['Method'] == method, 'Combined_Distance'].values[0]\n",
    "        plt.text(i, max(licit_scores[i], illicit_scores[i]) + 0.1, \n",
    "                 f'Dist: {combined_dist:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'normalized_similarity_top15.png'), dpi=300)\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing directory structure in: /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946\n",
      "Found base directories: ['method_comparisons', 'models', 'augmented', 'comparison']\n",
      "Loaded original metrics from /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946/original/graph_properties/graph_metrics.csv\n",
      "Found graph metrics for models/base_gat at: /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946/models/base_gat/graph_properties/graph_metrics.csv\n",
      "Processing metrics for: base_gat\n",
      "base_gat licit normalized similarity: 0.9564\n",
      "base_gat illicit normalized similarity: 0.7769\n",
      "Found graph metrics for models/base_sage at: /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946/models/base_sage/graph_properties/graph_metrics.csv\n",
      "Processing metrics for: base_sage\n",
      "base_sage licit normalized similarity: 1.2291\n",
      "base_sage illicit normalized similarity: 0.7882\n",
      "No metrics file found for augmented/label_propagation/gat\n",
      "No metrics file found for augmented/label_propagation/sage\n",
      "Found graph metrics for augmented/gat/gat at: /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946/augmented/gat/gat/graph_properties_with_model/graph_metrics.csv\n",
      "Processing metrics for: gat_gat\n",
      "gat_gat licit normalized similarity: 0.9947\n",
      "gat_gat illicit normalized similarity: 0.7756\n",
      "Found graph metrics for augmented/gat/sage at: /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946/augmented/gat/sage/graph_properties_with_model/graph_metrics.csv\n",
      "Processing metrics for: gat_sage\n",
      "gat_sage licit normalized similarity: 0.9861\n",
      "gat_sage illicit normalized similarity: 0.7786\n",
      "Found graph metrics for augmented/sage/gat at: /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946/augmented/sage/gat/graph_properties_with_model/graph_metrics.csv\n",
      "Processing metrics for: sage_gat\n",
      "sage_gat licit normalized similarity: 1.1134\n",
      "sage_gat illicit normalized similarity: 0.7760\n",
      "Found graph metrics for augmented/sage/sage at: /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946/augmented/sage/sage/graph_properties_with_model/graph_metrics.csv\n",
      "Processing metrics for: sage_sage\n",
      "sage_sage licit normalized similarity: 0.9129\n",
      "sage_sage illicit normalized similarity: 0.7768\n",
      "\n",
      "===== LICIT NODES: TOP 5 METHODS CLOSEST TO IDEAL SIMILARITY (1.0) =====\n",
      "gat_gat: 0.9947 (distance from ideal: 0.0053)\n",
      "gat_sage: 0.9861 (distance from ideal: 0.0139)\n",
      "base_gat: 0.9564 (distance from ideal: 0.0436)\n",
      "sage_sage: 0.9129 (distance from ideal: 0.0871)\n",
      "sage_gat: 1.1134 (distance from ideal: 0.1134)\n",
      "\n",
      "===== LICIT NODES: BOTTOM 5 METHODS FURTHEST FROM IDEAL SIMILARITY (1.0) =====\n",
      "gat_sage: 0.9861 (distance from ideal: 0.0139)\n",
      "base_gat: 0.9564 (distance from ideal: 0.0436)\n",
      "sage_sage: 0.9129 (distance from ideal: 0.0871)\n",
      "sage_gat: 1.1134 (distance from ideal: 0.1134)\n",
      "base_sage: 1.2291 (distance from ideal: 0.2291)\n",
      "\n",
      "===== ILLICIT NODES: TOP 5 METHODS CLOSEST TO IDEAL SIMILARITY (1.0) =====\n",
      "base_sage: 0.7882 (distance from ideal: 0.2118)\n",
      "gat_sage: 0.7786 (distance from ideal: 0.2214)\n",
      "base_gat: 0.7769 (distance from ideal: 0.2231)\n",
      "sage_sage: 0.7768 (distance from ideal: 0.2232)\n",
      "sage_gat: 0.7760 (distance from ideal: 0.2240)\n",
      "\n",
      "===== ILLICIT NODES: BOTTOM 5 METHODS FURTHEST FROM IDEAL SIMILARITY (1.0) =====\n",
      "gat_sage: 0.7786 (distance from ideal: 0.2214)\n",
      "base_gat: 0.7769 (distance from ideal: 0.2231)\n",
      "sage_sage: 0.7768 (distance from ideal: 0.2232)\n",
      "sage_gat: 0.7760 (distance from ideal: 0.2240)\n",
      "gat_gat: 0.7756 (distance from ideal: 0.2244)\n",
      "Visualizations saved to /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946/method_comparisons\n"
     ]
    }
   ],
   "source": [
    "results = post_processing('/Users/xuhuizhan/BTCGraphGuard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruct graph metrics from saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_graph_metrics(root_folder):\n",
    "    \"\"\"\n",
    "    Reconstructs graph metrics for missing files by rerunning label propagation\n",
    "    and training the models when needed.\n",
    "    \n",
    "    Args:\n",
    "        root_folder (str): Root folder of the project\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import torch\n",
    "    \n",
    "    # Define paths\n",
    "    analysis_dir = os.path.join(root_folder, 'output/comprehensive_analysis_20250410_201946')\n",
    "    \n",
    "    # Paths to process\n",
    "    target_paths = [\n",
    "        ('augmented/label_propagation/gat', 'GAT'),\n",
    "        ('augmented/label_propagation/sage', 'GraphSAGE')\n",
    "    ]\n",
    "    \n",
    "    # Create directories for metrics if they don't exist\n",
    "    for rel_path, _ in target_paths:\n",
    "        os.makedirs(os.path.join(analysis_dir, rel_path, 'graph_properties'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(analysis_dir, rel_path, 'graph_properties_with_model'), exist_ok=True)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Process each model configuration\n",
    "    for rel_path, model_type in target_paths:\n",
    "        print(f\"Processing {rel_path} with {model_type}\")\n",
    "        \n",
    "        try:\n",
    "            # First, run label propagation to get augmented data\n",
    "            print(\"Running label propagation to augment data...\")\n",
    "            augmented_data = augment_labels(data, percentage=0.3)\n",
    "            \n",
    "            # Save graph properties for augmented data\n",
    "            graph_props_dir = os.path.join(analysis_dir, rel_path, 'graph_properties')\n",
    "            print(f\"Analyzing graph properties for augmented data and saving to {graph_props_dir}...\")\n",
    "            analyze_graph_properties(\n",
    "                augmented_data,\n",
    "                save_dir=graph_props_dir,\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            # Define model based on model_type\n",
    "            model_args = {\n",
    "                'model_name': model_type,\n",
    "                'input_dim': augmented_data.x.shape[1],\n",
    "                'hidden_dim': 64,\n",
    "                'output_dim': len(torch.unique(augmented_data.y[augmented_data.y != 2])),\n",
    "                'heads': 8  # Only used for GAT\n",
    "            }\n",
    "            \n",
    "            # Train the model with the augmented data\n",
    "            print(f\"Training {model_type} model on augmented data...\")\n",
    "            model_save_path = os.path.join(analysis_dir, rel_path, f\"label_propagation_{model_type.lower()}_model.pt\")\n",
    "            \n",
    "            model_results = train_gnn_model(\n",
    "                graph_data=augmented_data,\n",
    "                checkpoint_path=model_save_path,\n",
    "                model_args=model_args,\n",
    "                num_epochs=30,\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            # Load the trained model\n",
    "            model = model_results['model']\n",
    "            \n",
    "            # Analyze graph properties with the trained model\n",
    "            graph_props_with_model_dir = os.path.join(analysis_dir, rel_path, 'graph_properties_with_model')\n",
    "            print(f\"Analyzing graph properties with trained model and saving to {graph_props_with_model_dir}...\")\n",
    "            analyze_graph_properties(\n",
    "                data,  # Use original data to get predictions on unknown nodes\n",
    "                model=model,\n",
    "                save_dir=graph_props_with_model_dir,\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            print(f\"Successfully processed {rel_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {rel_path}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    print(\"Reconstruction of missing metrics complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing augmented/label_propagation/gat with GAT\n",
      "Running label propagation to augment data...\n",
      "Using self-supervised learning for label augmentation\n",
      "Added 47161 previously unknown nodes to training set:\n",
      "  - Predicted illicit: 45741 (96.99%)\n",
      "  - Predicted licit: 1420 (3.01%)\n",
      "Analyzing graph properties for augmented data and saving to /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946/augmented/label_propagation/gat/graph_properties...\n",
      "Processed train_licit: 5022 nodes, 2172 edges\n",
      "Processed train_illicit: 79390 nodes, 77956 edges\n",
      "Processed val_licit: 508 nodes, 18 edges\n",
      "Processed val_illicit: 4148 nodes, 379 edges\n",
      "Processed test_licit: 435 nodes, 8 edges\n",
      "Processed test_illicit: 4222 nodes, 394 edges\n",
      "Processed unknown: 110044 nodes, 86291 edges\n",
      "Analysis complete. Results saved to /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946/augmented/label_propagation/gat/graph_properties\n",
      "Training GAT model on augmented data...\n",
      "GAT Epoch: 010, Loss: 0.8048, Train Acc: 0.8726, Val Acc: 0.8862\n",
      "GAT Epoch: 020, Loss: 0.6184, Train Acc: 0.5846, Val Acc: 0.6329\n",
      "GAT Epoch: 030, Loss: 0.4959, Train Acc: 0.6746, Val Acc: 0.7459\n",
      "GAT Test Accuracy: 0.7533\n",
      "GAT Training Time: 63.99 seconds\n",
      "Analyzing graph properties with trained model and saving to /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946/augmented/label_propagation/gat/graph_properties_with_model...\n",
      "Processed train_licit: 3602 nodes, 651 edges\n",
      "Processed train_illicit: 33649 nodes, 21207 edges\n",
      "Processed val_licit: 508 nodes, 18 edges\n",
      "Processed val_illicit: 4148 nodes, 379 edges\n",
      "Processed test_licit: 435 nodes, 8 edges\n",
      "Processed test_illicit: 4222 nodes, 394 edges\n",
      "Processed pred_unknown_licit: 87099 nodes, 62303 edges\n",
      "Processed pred_unknown_illicit: 70106 nodes, 56772 edges\n",
      "Unknown nodes (157205) predicted as:\n",
      "  - Licit: 87099 (55.40%)\n",
      "  - Illicit: 70106 (44.60%)\n",
      "Analysis complete. Results saved to /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946/augmented/label_propagation/gat/graph_properties_with_model\n",
      "Successfully processed augmented/label_propagation/gat\n",
      "Processing augmented/label_propagation/sage with GraphSAGE\n",
      "Running label propagation to augment data...\n",
      "Using self-supervised learning for label augmentation\n",
      "Added 47161 previously unknown nodes to training set:\n",
      "  - Predicted illicit: 45741 (96.99%)\n",
      "  - Predicted licit: 1420 (3.01%)\n",
      "Analyzing graph properties for augmented data and saving to /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946/augmented/label_propagation/sage/graph_properties...\n",
      "Processed train_licit: 5022 nodes, 2172 edges\n",
      "Processed train_illicit: 79390 nodes, 77956 edges\n",
      "Processed val_licit: 508 nodes, 18 edges\n",
      "Processed val_illicit: 4148 nodes, 379 edges\n",
      "Processed test_licit: 435 nodes, 8 edges\n",
      "Processed test_illicit: 4222 nodes, 394 edges\n",
      "Processed unknown: 110044 nodes, 86291 edges\n",
      "Analysis complete. Results saved to /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946/augmented/label_propagation/sage/graph_properties\n",
      "Training GraphSAGE model on augmented data...\n",
      "GraphSAGE Epoch: 010, Loss: 0.2582, Train Acc: 0.9405, Val Acc: 0.8909\n",
      "GraphSAGE Epoch: 020, Loss: 0.1945, Train Acc: 0.9414, Val Acc: 0.8935\n",
      "GraphSAGE Epoch: 030, Loss: 0.1688, Train Acc: 0.9407, Val Acc: 0.8913\n",
      "GraphSAGE Test Accuracy: 0.9068\n",
      "GraphSAGE Training Time: 11.50 seconds\n",
      "Analyzing graph properties with trained model and saving to /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946/augmented/label_propagation/sage/graph_properties_with_model...\n",
      "Processed train_licit: 3602 nodes, 651 edges\n",
      "Processed train_illicit: 33649 nodes, 21207 edges\n",
      "Processed val_licit: 508 nodes, 18 edges\n",
      "Processed val_illicit: 4148 nodes, 379 edges\n",
      "Processed test_licit: 435 nodes, 8 edges\n",
      "Processed test_illicit: 4222 nodes, 394 edges\n",
      "Processed pred_unknown_licit: 30 nodes, 0 edges\n",
      "Processed pred_unknown_illicit: 157175 nodes, 131740 edges\n",
      "Unknown nodes (157205) predicted as:\n",
      "  - Licit: 30 (0.02%)\n",
      "  - Illicit: 157175 (99.98%)\n",
      "Analysis complete. Results saved to /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946/augmented/label_propagation/sage/graph_properties_with_model\n",
      "Successfully processed augmented/label_propagation/sage\n",
      "Reconstruction of missing metrics complete!\n",
      "Analyzing directory structure in: /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946\n",
      "Found base directories: ['method_comparisons', 'models', 'augmented', 'comparison']\n",
      "Loaded original metrics from /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946/original/graph_properties/graph_metrics.csv\n",
      "Found graph metrics for models/base_gat at: /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946/models/base_gat/graph_properties/graph_metrics.csv\n",
      "Processing metrics for: base_gat\n",
      "base_gat licit normalized similarity: 0.9564\n",
      "base_gat illicit normalized similarity: 0.7769\n",
      "Found graph metrics for models/base_sage at: /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946/models/base_sage/graph_properties/graph_metrics.csv\n",
      "Processing metrics for: base_sage\n",
      "base_sage licit normalized similarity: 1.2291\n",
      "base_sage illicit normalized similarity: 0.7882\n",
      "Found graph metrics for augmented/label_propagation/gat at: /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946/augmented/label_propagation/gat/graph_properties/graph_metrics.csv\n",
      "Processing metrics for: label_propagation_gat\n",
      "Warning: pred_unknown_licit not found in label_propagation_gat\n",
      "Warning: pred_unknown_illicit not found in label_propagation_gat\n",
      "Skipping label_propagation_gat as it has no pred_unknown entries\n",
      "Found graph metrics for augmented/label_propagation/sage at: /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946/augmented/label_propagation/sage/graph_properties/graph_metrics.csv\n",
      "Processing metrics for: label_propagation_sage\n",
      "Warning: pred_unknown_licit not found in label_propagation_sage\n",
      "Warning: pred_unknown_illicit not found in label_propagation_sage\n",
      "Skipping label_propagation_sage as it has no pred_unknown entries\n",
      "Found graph metrics for augmented/gat/gat at: /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946/augmented/gat/gat/graph_properties_with_model/graph_metrics.csv\n",
      "Processing metrics for: gat_gat\n",
      "gat_gat licit normalized similarity: 0.9947\n",
      "gat_gat illicit normalized similarity: 0.7756\n",
      "Found graph metrics for augmented/gat/sage at: /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946/augmented/gat/sage/graph_properties_with_model/graph_metrics.csv\n",
      "Processing metrics for: gat_sage\n",
      "gat_sage licit normalized similarity: 0.9861\n",
      "gat_sage illicit normalized similarity: 0.7786\n",
      "Found graph metrics for augmented/sage/gat at: /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946/augmented/sage/gat/graph_properties_with_model/graph_metrics.csv\n",
      "Processing metrics for: sage_gat\n",
      "sage_gat licit normalized similarity: 1.1134\n",
      "sage_gat illicit normalized similarity: 0.7760\n",
      "Found graph metrics for augmented/sage/sage at: /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946/augmented/sage/sage/graph_properties_with_model/graph_metrics.csv\n",
      "Processing metrics for: sage_sage\n",
      "sage_sage licit normalized similarity: 0.9129\n",
      "sage_sage illicit normalized similarity: 0.7768\n",
      "\n",
      "===== LICIT NODES: TOP 5 METHODS CLOSEST TO IDEAL SIMILARITY (1.0) =====\n",
      "gat_gat: 0.9947 (distance from ideal: 0.0053)\n",
      "gat_sage: 0.9861 (distance from ideal: 0.0139)\n",
      "base_gat: 0.9564 (distance from ideal: 0.0436)\n",
      "sage_sage: 0.9129 (distance from ideal: 0.0871)\n",
      "sage_gat: 1.1134 (distance from ideal: 0.1134)\n",
      "\n",
      "===== LICIT NODES: BOTTOM 5 METHODS FURTHEST FROM IDEAL SIMILARITY (1.0) =====\n",
      "gat_sage: 0.9861 (distance from ideal: 0.0139)\n",
      "base_gat: 0.9564 (distance from ideal: 0.0436)\n",
      "sage_sage: 0.9129 (distance from ideal: 0.0871)\n",
      "sage_gat: 1.1134 (distance from ideal: 0.1134)\n",
      "base_sage: 1.2291 (distance from ideal: 0.2291)\n",
      "\n",
      "===== ILLICIT NODES: TOP 5 METHODS CLOSEST TO IDEAL SIMILARITY (1.0) =====\n",
      "base_sage: 0.7882 (distance from ideal: 0.2118)\n",
      "gat_sage: 0.7786 (distance from ideal: 0.2214)\n",
      "base_gat: 0.7769 (distance from ideal: 0.2231)\n",
      "sage_sage: 0.7768 (distance from ideal: 0.2232)\n",
      "sage_gat: 0.7760 (distance from ideal: 0.2240)\n",
      "\n",
      "===== ILLICIT NODES: BOTTOM 5 METHODS FURTHEST FROM IDEAL SIMILARITY (1.0) =====\n",
      "gat_sage: 0.7786 (distance from ideal: 0.2214)\n",
      "base_gat: 0.7769 (distance from ideal: 0.2231)\n",
      "sage_sage: 0.7768 (distance from ideal: 0.2232)\n",
      "sage_gat: 0.7760 (distance from ideal: 0.2240)\n",
      "gat_gat: 0.7756 (distance from ideal: 0.2244)\n",
      "Visualizations saved to /Users/xuhuizhan/BTCGraphGuard/output/comprehensive_analysis_20250410_201946/method_comparisons\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reconstruct_graph_metrics(root_folder='/Users/xuhuizhan/BTCGraphGuard')\n",
    "results = post_processing('/Users/xuhuizhan/BTCGraphGuard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
